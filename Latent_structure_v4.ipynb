{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iliaMalinovskii/MultiGrid-Transformer/blob/main/Latent_structure_v4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "rnc8KAdyM2mV",
        "outputId": "3b5f7fec-52fe-4e4f-d620-fb6a94fa7314"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "source": [
        "# @title Default title text\n",
        "import copy\n",
        "import random\n",
        "import itertools\n",
        "\n",
        "!pip install nltk\n",
        "import nltk\n",
        "import os\n",
        "from google.colab import drive\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "import spacy\n",
        "\n",
        "import datetime\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "#...\n",
        "\n",
        "nltk.download('averaged_perceptron_tagger', download_dir='/root/nltk_data')\n",
        "nltk.data.path.append('/root/nltk_data') # Tell nltk to include the new directory in the search path\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize, RegexpTokenizer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "import re\n",
        "\n",
        "#...\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class Workflow:\n",
        "  def __init__(self):\n",
        "    self.log = {}\n",
        "    self.alltext =\"\"\n",
        "    self.decode_dict={}\n",
        "    self.pos_list =[]\n",
        "    self.list_of_seq = []\n",
        "    self.data = []\n",
        "    self.train_data = []\n",
        "    self.val_data = []\n",
        "    self.vocab_sizes = []\n",
        "    return\n",
        "\n",
        "  def load_texts(self, max_text = 5, base_tokens_dict = None,\n",
        "                 drive_mount_path = '/content/drive',\n",
        "                 folder_path = '/content/drive/MyDrive/Colab Notebooks/fairy_tales'):\n",
        "\n",
        "    drive.mount(drive_mount_path)\n",
        "    text_count = 0\n",
        "    base_token_seq = []\n",
        "    for filename in os.listdir(folder_path):\n",
        "      if filename.endswith(\".txt\"):\n",
        "        filepath = os.path.join(folder_path, filename)\n",
        "        try:\n",
        "          with open(filepath, 'r', encoding='utf-8',errors='ignore') as f:\n",
        "            file_text = f.read().lower() # TODO: remove first row, that is title\n",
        "            file_text = ' '.join(file_text.strip().split())\n",
        "            file_text = re.sub(r\"\\s+\", \" \", file_text).strip()\n",
        "            if text_count >0: file_text = \" \"+file_text\n",
        "            prev_t = None\n",
        "            for t in file_text:\n",
        "              token = base_tokens_dict.get(t)\n",
        "              if token == None: continue\n",
        "              if t == \" \" and prev_t == \" \": continue\n",
        "              base_token_seq.append(t)\n",
        "              prev_t = t\n",
        "            text_count += 1\n",
        "            if (100*(text_count/max_text)) % 20 == 0: print(f\"{text_count} texts loaded\")\n",
        "            if text_count >= max_text: break\n",
        "        except Exception as e:\n",
        "          print(f\"Error reading file {filename}: {e}\")\n",
        "\n",
        "    self.alltext = ''.join(base_token_seq)\n",
        "    return base_token_seq\n",
        "\n",
        "  def dictionaries(self):\n",
        "    list_of_dicts = {}\n",
        "\n",
        "    # Dictionary #1 MAIN\n",
        "    alphabet_value = 0\n",
        "    alphabet_dict = {}\n",
        "    alphabet = 'abcdefghijklmnopqrstuvwxyz0123456789 '\n",
        "    for t in alphabet:\n",
        "      alphabet_dict[t] = alphabet_value\n",
        "      self.decode_dict[alphabet_value] = t\n",
        "      alphabet_value += 1\n",
        "\n",
        "    list_of_dicts[\"MAIN\"] = alphabet_dict\n",
        "    # Dictionary #2\n",
        "    morphemes_value = 1\n",
        "    morphemes_dict = {}\n",
        "    morphemes_list = [\"ab\", \"ad\", \"ante\", \"anti\", \"auto\", \"ation\", \"ative\",\n",
        "                      \"be\", \"bi\", \"circum\", \"co\", \"com\", \"con\", \"counter\",\n",
        "                      \"de\", \"dis\",\"em\", \"en\", \"epi\", \"es\", \"eu\", \"ex\", \"extra\",\n",
        "                      \"hyper\", \"hypo\", \"ible\", \"il\", \"im\", \"in\", \"inter\",\n",
        "                      \"intra\",\"ion\", \"ir\", \"ise\", \"iso\", \"ition\", \"itive\",\n",
        "                      \"mal\",\"mid\", \"mis\", \"mono\", \"non\", \"ob\", \"omni\",\"or\",\n",
        "                      \"out\", \"over\", \"post\", \"pre\", \"pro\", \"re\", \"semi\",\n",
        "                      \"sub\", \"super\", \"trans\", \"ty\", \"un\", \"under\", \"uni\",\n",
        "                      \"vice\", \"ward\", \"with\", \"wise\", \"able\", \"al\", \"ance\",\n",
        "                      \"ant\", \"ary\", \"ate\", \"dom\", \"ed\", \"ence\", \"ency\", \"er\",\n",
        "                      \"est\", \"eous\", \"fore,\" \"ful\", \"fy\", \"hood\", \"ic\", \"ical\",\n",
        "                      \"ial\", \"ify\", \"ing\", \"ious\", \"ism\", \"ist\", \"ity\", \"ive\",\n",
        "                      \"ize\", \"less\", \"ly\", \"ment\", \"ness\", \"ous\", \"ship\",\n",
        "                      \"sion\", \"tion\", \"ure\"]\n",
        "\n",
        "    for m in morphemes_list:\n",
        "      morphemes_dict[m] = morphemes_value\n",
        "      morphemes_value += 1\n",
        "\n",
        "    list_of_dicts[\"MORPH\"] = morphemes_dict\n",
        "    return list_of_dicts\n",
        "\n",
        "  def pos_dict(self, default_value = 0):\n",
        "\n",
        "    # Dictionary #3\n",
        "\n",
        "    self.pos_list = [\"CC\", #coordinating conjunction\n",
        "      \"CD\", #cardinal digit\n",
        "      \"DT\", #determiner\n",
        "      \"EX\", #existential there (like: “there is” … think of it like “there exists”)\n",
        "      \"FW\", #foreign word\n",
        "      \"IN\", #preposition/subordinating conjunction\n",
        "      \"JJ\", # adjective – ‘big’\n",
        "      \"JJR\", # adjective, comparative – ‘bigger’\n",
        "      \"JJS\", # adjective, superlative – ‘biggest’\n",
        "      \"LS\", # list marker 1)\n",
        "      \"MD\", # modal – could, will\n",
        "      \"NN\", # noun, singular ‘- desk’\n",
        "      \"NNS\", # noun plural – ‘desks’\n",
        "      \"NNP\", # proper noun, singular – ‘Harrison’\n",
        "      \"NNPS\", # proper noun, plural – ‘Americans’\n",
        "      \"PDT\", # predeterminer – ‘all the kids’\n",
        "      \"POS\", # possessive ending parent’s\n",
        "      \"PRP\", # personal pronoun –  I, he, she\n",
        "      \"PRP$\", # possessive pronoun – my, his, hers\n",
        "      \"RB\", # adverb – very, silently,\n",
        "      \"RBR\", # adverb, comparative – better\n",
        "      \"RBS\", # adverb, superlative – best\n",
        "      \"RP\", # particle – give up\n",
        "      \"TO\", # – to go ‘to’ the store.\n",
        "      \"UH\", # interjection – errrrrrrrm\n",
        "      \"VB\", # verb, base form – take\n",
        "      \"VBD\", # verb, past tense – took\n",
        "      \"VBG\", # verb, gerund/present participle – taking\n",
        "      \"VBN\", # verb, past participle – taken\n",
        "      \"VBP\", # verb, sing. present, non-3d – take\n",
        "      \"VBZ\", # verb, 3rd person sing. present – takes\n",
        "      \"WDT\", # wh-determiner – which\n",
        "      \"WP\", # wh-pronoun – who, what\n",
        "      \"WP$\", # possessive wh-pronoun, eg- whose\n",
        "      \"WRB\"] # wh-adverb, eg- where, when\n",
        "\n",
        "    pos_dict = {}\n",
        "    pos_dict_value = 1\n",
        "    for p in self.pos_list:\n",
        "      pos_dict[p] = pos_dict_value\n",
        "      pos_dict_value += 1\n",
        "\n",
        "    tokenized = self.alltext.split()\n",
        "    pos_seq = nltk.pos_tag(tokenized)\n",
        "    pos_tokenized = []\n",
        "    j=0\n",
        "    for t in pos_seq:\n",
        "      j+=1\n",
        "      for i in range(len(t[0])):\n",
        "        pos_tag_value = pos_dict.get(t[1])\n",
        "        if pos_tag_value is None: pos_tag_value = default_value\n",
        "        pos_tokenized.append(pos_tag_value)\n",
        "      if j < len(pos_seq): pos_tokenized.append(default_value)\n",
        "\n",
        "    return pos_tokenized\n",
        "\n",
        "  def translate(dictionary, tokens, default_value=0):\n",
        "    result = [default_value] * len(tokens)\n",
        "    k=0\n",
        "    for key in sorted(dictionary, key=len):\n",
        "      key_len = len(key)\n",
        "      if k % 50 == 0: print(k)\n",
        "\n",
        "      k+=1\n",
        "      i=0\n",
        "      while i < len(tokens) - key_len + 1:\n",
        "        if result[i] != default_value:\n",
        "          i+=1\n",
        "          continue\n",
        "        subsequence = ''.join(tokens[i:i + key_len])\n",
        "        if subsequence == key:\n",
        "          for j in range(key_len):\n",
        "            result[i + j] = dictionary[key]\n",
        "          i+=j+1\n",
        "        else: i+=1\n",
        "    return result\n",
        "\n",
        "  def data_prep(self, max_text = 5, default_value = 0, load_seq = None):\n",
        "    n = -1\n",
        "    i = 0\n",
        "    for l in load_seq:\n",
        "      match l:\n",
        "        case \"MAIN\":\n",
        "          self.list_of_dicts = self.dictionaries()\n",
        "          self.base_token_seq = self.load_texts(max_text = max_text, base_tokens_dict = self.list_of_dicts.get(\"MAIN\"))\n",
        "          self.list_of_seq.append(Workflow.translate(self.list_of_dicts.get(\"MAIN\"), self.base_token_seq, default_value=default_value))\n",
        "          self.data.append(torch.tensor(self.list_of_seq[i], dtype=torch.long))\n",
        "          self.vocab_sizes.append(len(self.list_of_dicts.get(\"MAIN\")))\n",
        "          n = int(0.9*len(self.data[i]))\n",
        "          self.train_data.append(self.data[i][:n])\n",
        "          self.val_data.append(self.data[i][n:])\n",
        "          i+=1\n",
        "        case \"POS\":\n",
        "          pos_seq = self.pos_dict()\n",
        "          self.list_of_seq.append(pos_seq)\n",
        "          self.vocab_sizes.append(101)\n",
        "          self.data.append(torch.tensor(self.list_of_seq[i], dtype=torch.long))\n",
        "          self.train_data.append(self.data[i][:n])\n",
        "          self.val_data.append(self.data[i][n:])\n",
        "          i+=1\n",
        "        case \"MORPH\":\n",
        "          self.list_of_seq.append(Workflow.translate(self.list_of_dicts.get(\"MORPH\"),self.base_token_seq,default_value = default_value))\n",
        "          self.data.append(torch.tensor(self.list_of_seq[i], dtype=torch.long))\n",
        "          self.vocab_sizes.append(max(list(self.list_of_dicts.get(\"MORPH\").values()))+2)\n",
        "          self.train_data.append(self.data[i][:n])\n",
        "          self.val_data.append(self.data[i][n:])\n",
        "          i+=1\n",
        "    return\n",
        "\n",
        "  def get_batch(self, split):\n",
        "    data = self.train_data if split == 'train' else self.val_data\n",
        "    ix = torch.randint(len(data[0]) - block_size-1, (batch_size,))\n",
        "    x = []\n",
        "    y = []\n",
        "    for j in range(len(data)):\n",
        "        x.append(torch.stack([data[j][i : min(i + block_size, len(data[j]))] for i in ix]))\n",
        "        y.append(torch.stack([data[j][i + 1 : min(i + 1 + block_size, len(data[j]))] for i in ix]))\n",
        "        x[j], y[j] = x[j].to(device), y[j].to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "  out = {}\n",
        "  out_detailed = {}\n",
        "  ind_losses = torch.zeros((input_dim, eval_iters))\n",
        "  model.eval()\n",
        "  for split in ['train', 'val']:\n",
        "    losses = torch.zeros(eval_iters)\n",
        "    for k in range(eval_iters):\n",
        "      X, Y = w.get_batch(split)\n",
        "      logits, loss, individual_losses = model(X, Y)\n",
        "      losses[k] = loss.item()\n",
        "      for i in range(input_dim):\n",
        "        ind_losses[i][k] = individual_losses[i].item()\n",
        "    out[split] = losses.mean()\n",
        "    i_losses = []\n",
        "    for i in range(input_dim): i_losses.append(ind_losses[i].mean())\n",
        "    out_detailed[split] = i_losses #(ind_losses[0].mean(),ind_losses[1].mean(),ind_losses[2].mean())\n",
        "  model.train()\n",
        "  return out, out_detailed\n",
        "\n",
        "class Head(nn.Module):\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # input of size (batch, T = block size ... seq length, C=embedding size)\n",
        "        # output of size (batch, time-step, head size)\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x) # (B,T,hs)\n",
        "        q = self.query(x) # (B,T,hs)\n",
        "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        v = self.value(x) # (B,T,hs)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
        "        return out\n",
        "\n",
        "class HeadXAttn(nn.Module):\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, w):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(w)\n",
        "        q = self.query(x) # (B,T,hs)\n",
        "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        v = self.value(w) # (B,T,hs)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class MultiHeadXAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([HeadXAttn(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, w):\n",
        "        out = torch.cat([h(x,w) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, n_embd, n_head, num_frames):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "\n",
        "        self.ln_t1 = nn.LayerNorm(n_embd)\n",
        "        self.sa_t = MultiHeadAttention(n_head, head_size)\n",
        "        self.ln_t2 = nn.LayerNorm(n_embd)\n",
        "        self.ffwd_t = FeedFoward(n_embd)\n",
        "\n",
        "        self.ln_f1 = nn.ModuleList([nn.LayerNorm(n_embd) for _ in range(num_frames)])\n",
        "        self.sa_f = nn.ModuleList([MultiHeadAttention(n_head, head_size) for _ in range(num_frames)])\n",
        "        self.ln_f2 = nn.ModuleList([nn.LayerNorm(n_embd) for _ in range(num_frames)])\n",
        "        self.ffwd_f = nn.ModuleList([FeedFoward(n_embd) for _ in range(num_frames)])\n",
        "\n",
        "        self.ln_x1 = nn.ModuleList([nn.LayerNorm(n_embd) for _ in range(num_frames)])\n",
        "        self.ln_w1 = nn.ModuleList([nn.LayerNorm(n_embd) for _ in range(num_frames)])\n",
        "        self.sa_x = nn.ModuleList([MultiHeadXAttention(n_head, head_size) for _ in range(num_frames)])\n",
        "        self.ln_x2 = nn.ModuleList([nn.LayerNorm(n_embd) for _ in range(num_frames)])\n",
        "        self.ffwd_x = nn.ModuleList([FeedFoward(n_embd) for _ in range(num_frames)])\n",
        "\n",
        "        self.sa_w = nn.ModuleList([MultiHeadXAttention(n_head, head_size) for _ in range(num_frames)])\n",
        "        self.ln_w2 = nn.ModuleList([nn.LayerNorm(n_embd) for _ in range(num_frames)])\n",
        "        self.ffwd_w = nn.ModuleList([FeedFoward(n_embd) for _ in range(num_frames)])\n",
        "\n",
        "    def forward(self, x, w):\n",
        "        x = x + self.sa_t(self.ln_t1(x))\n",
        "        x = x + self.ffwd_t(self.ln_t2(x))\n",
        "        # Stack w into a single tensor\n",
        "        #w = torch.stack(w, dim=0) # (num_frames, B, T, C)\n",
        "\n",
        "        # Apply layers to the stacked w tensor\n",
        "        if w is not None:\n",
        "          w = w + torch.stack([sa_f(ln_f1(w_frame))\n",
        "                            for sa_f, ln_f1, w_frame in zip(self.sa_f, self.ln_f1, w)], dim=0)\n",
        "          w = w + torch.stack([ffwd_f(ln_f2(w_frame))\n",
        "                            for ffwd_f, ln_f2, w_frame in zip(self.ffwd_f, self.ln_f2, w)], dim=0)\n",
        "\n",
        "        # Repeat x along the num_frames dimension\n",
        "        x_shape = x.shape\n",
        "        if w is not None:\n",
        "          x = x.unsqueeze(0).repeat(w.shape[0], 1, 1, 1) # (num_frames, B, T, C)\n",
        "\n",
        "        # Apply cross-attention and feedforward layers\n",
        "        if w is not None:\n",
        "          x = x + torch.stack([sa_x(ln_x1(x_frame), ln_w1(w_frame))\n",
        "                            for sa_x, ln_x1, ln_w1, x_frame, w_frame in zip(self.sa_x, self.ln_x1, self.ln_w1, x, w)], dim=0)\n",
        "          x = x + torch.stack([ffwd_x(ln_x2(x_frame))\n",
        "                            for ffwd_x, ln_x2, x_frame in zip(self.ffwd_x, self.ln_x2, x)], dim=0)\n",
        "\n",
        "        if w is not None:\n",
        "          w = w + torch.stack([sa_w(ln_w1(w_frame), ln_x1(x_frame))\n",
        "                            for sa_w, ln_w1, ln_x1, w_frame, x_frame in zip(self.sa_w, self.ln_w1, self.ln_x1, w, x)], dim=0)\n",
        "          w = w + torch.stack([ffwd_w(ln_w2(w_frame))\n",
        "                            for ffwd_w, ln_w2, w_frame in zip(self.ffwd_w, self.ln_w2, w)], dim=0)\n",
        "\n",
        "          # Average x across the num_frames dimension\n",
        "          x = torch.mean(x, dim=0)\n",
        "\n",
        "        return x, w\n",
        "\n",
        "class GPTLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_sizes):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_sizes[0], n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_sizes[0])\n",
        "\n",
        "        self.frame_embedding_table = []\n",
        "        self.frame_position_emb_table = []\n",
        "        self.ln_w = []\n",
        "        self.lm_head_w = []\n",
        "\n",
        "        self.frame_embedding_table = nn.ModuleList([nn.Embedding(v, n_embd) for v in vocab_sizes[1:]])\n",
        "        self.frame_position_emb_table = nn.ModuleList([nn.Embedding(block_size, n_embd) for _ in range(len(vocab_sizes) - 1)])\n",
        "        self.ln_w = nn.ModuleList([nn.LayerNorm(n_embd) for _ in range(len(vocab_sizes) - 1)])\n",
        "        self.lm_head_w = nn.ModuleList([nn.Linear(n_embd, v) for v in vocab_sizes[1:]])\n",
        "\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head,num_frames=len(vocab_sizes)-1) for _ in range(n_layer)])\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx[0].shape # w is of the same size\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx[0]) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "\n",
        "        if input_dim >1:\n",
        "          w = torch.stack([emb(i) + pos(torch.arange(T, device=device))\n",
        "            for emb, pos, i in zip(self.frame_embedding_table, self.frame_position_emb_table, idx[1:])], dim=0)\n",
        "        else: w = None\n",
        "\n",
        "        for block in self.blocks:\n",
        "          x, w = block(x,w) # (B,T,C)\n",
        "\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if len(idx)>1:\n",
        "          logits_w = torch.stack([lm_head(ln(w_frame))\n",
        "            for lm_head, ln, w_frame in zip(self.lm_head_w, self.ln_w, w)], dim=0)\n",
        "        else: logits_w = None\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets_m = targets[0].view(B*T)\n",
        "            if len(idx)>1:\n",
        "              logits_w_reshaped = [lw.view(B*T, lw.shape[-1]) for lw in logits_w]\n",
        "              target_w = [t.view(B*T) for t in targets[1:]]\n",
        "\n",
        "            individual_losses = [F.cross_entropy(logits, targets_m)]\n",
        "\n",
        "            if len(idx)>1:\n",
        "              individual_losses.extend([F.cross_entropy(lw, tw) for lw, tw in zip(logits_w_reshaped, target_w)])\n",
        "              loss = main_seq_loss_contribution_weight * individual_losses[0] + (1-main_seq_loss_contribution_weight) * sum(individual_losses[1:]) / len(individual_losses[1:])\n",
        "            else:\n",
        "              loss = individual_losses[0]\n",
        "\n",
        "        return (logits,logits_w), loss, individual_losses\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "          idx_cond = []\n",
        "          for i in range(len(idx)):\n",
        "            idx_cond.append(idx[i][:, -block_size:])\n",
        "            ##idx_cond.append(idx[1][:, -block_size:])\n",
        "            # get the predictions\n",
        "          logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "          logits_m = []\n",
        "          probs =[]\n",
        "          idx_next_m = []\n",
        "          for i in range(len(idx)):\n",
        "            logits_m.append(logits[0][:, -1, :]) # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs.append(F.softmax(logits_m[i], dim=-1)) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next_m.append(torch.multinomial(probs[i], num_samples=1)) # (B, 1)\n",
        "          # append sampled index to the running sequence\n",
        "          idx_n = []\n",
        "          for i in range(len(idx)):\n",
        "            idx_n.append(torch.cat((idx[i], idx_next_m[i]), dim=1)) # (B, T+1)\n",
        "          idx = idx_n\n",
        "        return idx\n",
        "\n",
        "def save_dict_to_csv_gdrive(dictionary, filename, folder_path='/content/drive/My Drive/Colab Notebooks/'):\n",
        "  \"\"\"Saves a dictionary to a CSV file in Google Drive.\n",
        "\n",
        "  Args:\n",
        "      dictionary: The dictionary to save.\n",
        "      filename: The name of the CSV file.\n",
        "      folder_path: The path to the folder in Google Drive where the file should be saved.\n",
        "  \"\"\"\n",
        "\n",
        "  drive.mount('/content/drive')\n",
        "  filepath = os.path.join(folder_path, filename)\n",
        "\n",
        "  with open(filepath, 'a', newline='') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    for key, value in dictionary.items():\n",
        "      writer.writerow([key, value])\n",
        "\n",
        "\n",
        "# ------ hyperparameters ------------\n",
        "batch_size = 4#64#32#16 #8 #64 # how many independent sequences will we process in parallel?\n",
        "block_size = 128#128#256#128#64 #128 #256 # what is the maximum context length for predictions\n",
        "max_iters =  5000#40000 # 5000 #1000 #5000\n",
        "eval_interval = 250#1000 #100 #500\n",
        "learning_rate = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "eval_iters = 200\n",
        "n_embd = 48#16#64#32#16# 8 #384. IMPORTANT n_embd // n_head\n",
        "n_head = 16#6#4# 2 #6\n",
        "n_layer = 5#7 #6\n",
        "dropout = 0.2\n",
        "seq_order = ['MAIN','MORPH','POS']\n",
        "input_dim = len(seq_order)\n",
        "max_texts = 10 #1700\n",
        "main_seq_loss_contribution_weight = 0.8\n",
        "\n",
        "# ------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "QEpRaxeTxfdV",
        "outputId": "36b92a99-41dd-459d-d611-c514fcb3ad87"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "2 texts loaded\n",
            "4 texts loaded\n",
            "6 texts loaded\n",
            "8 texts loaded\n",
            "10 texts loaded\n",
            "0\n",
            "0\n",
            "50\n",
            "1.026383 M parameters\n",
            "step 0: train loss 3.8299, val loss 3.8290\n",
            "step 0: train loss 3.6258, val loss 3.6244\n",
            "step 0: train loss 4.6472, val loss 4.6478\n",
            "step 0: train loss 4.6455, val loss 4.6466\n",
            "step 250: train loss 2.2350, val loss 2.2206\n",
            "step 250: train loss 2.4302, val loss 2.4168\n",
            "step 250: train loss 1.2201, val loss 1.2111\n",
            "step 250: train loss 1.6877, val loss 1.6601\n",
            "step 500: train loss 2.0534, val loss 2.0485\n",
            "step 500: train loss 2.2945, val loss 2.2905\n",
            "step 500: train loss 1.0203, val loss 1.0318\n",
            "step 500: train loss 1.1576, val loss 1.1294\n",
            "step 750: train loss 1.9517, val loss 1.9380\n",
            "step 750: train loss 2.1938, val loss 2.1826\n",
            "step 750: train loss 0.9510, val loss 0.9384\n",
            "step 750: train loss 1.0150, val loss 0.9809\n",
            "step 1000: train loss 1.8051, val loss 1.8077\n",
            "step 1000: train loss 2.0294, val loss 2.0372\n",
            "step 1000: train loss 0.8983, val loss 0.8945\n",
            "step 1000: train loss 0.9170, val loss 0.8850\n",
            "step 1250: train loss 1.6535, val loss 1.6678\n",
            "step 1250: train loss 1.8554, val loss 1.8777\n",
            "step 1250: train loss 0.8511, val loss 0.8455\n",
            "step 1250: train loss 0.8401, val loss 0.8111\n",
            "step 1500: train loss 1.5537, val loss 1.5530\n",
            "step 1500: train loss 1.7364, val loss 1.7415\n",
            "step 1500: train loss 0.8384, val loss 0.8153\n",
            "step 1500: train loss 0.8074, val loss 0.7830\n"
          ]
        }
      ],
      "source": [
        "_save_needed = False\n",
        "\n",
        "w = Workflow()\n",
        "\n",
        "w.log[\"new_model_start\"] = (datetime.datetime.now().year,\n",
        "                            datetime.datetime.now().month,\n",
        "                            datetime.datetime.now().day,\n",
        "                            datetime.datetime.now().hour,\n",
        "                            datetime.datetime.now().minute)\n",
        "\n",
        "w.data_prep(max_text=max_texts, load_seq = seq_order)\n",
        "\n",
        "w.log[\"memo\"] = \"test\"\n",
        "w.log[\"batch_size\"] = batch_size\n",
        "w.log[\"block_size\"] = block_size\n",
        "w.log[\"max_iters\"] = max_iters\n",
        "w.log[\"eval_interval\"] = eval_interval\n",
        "w.log[\"learning_rate\"] = learning_rate\n",
        "w.log[\"device\"] = device\n",
        "w.log[\"eval_iters\"] = eval_iters\n",
        "w.log[\"n_embd\"] = n_embd\n",
        "w.log[\"n_head\"] = n_head\n",
        "w.log[\"n_layer\"] = n_layer\n",
        "w.log[\"dropout\"] = dropout\n",
        "w.log[\"max_texts\"] = max_texts\n",
        "\n",
        "w.log[\"vocab_size\"] = w.vocab_sizes\n",
        "\n",
        "model = GPTLanguageModel(w.vocab_sizes)\n",
        "m = model.to(device)\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses, lossess_detailed = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "        for i in range(input_dim): print(f\"step {iter}: train loss {lossess_detailed['train'][i]:.4f}, val loss {lossess_detailed['val'][i]:.4f}\")\n",
        "\n",
        "    x, y = w.get_batch('train')\n",
        "    (logits, logits_w), loss, individual_losses = model(x, y)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "idx = []\n",
        "idx.append(torch.zeros((1, 1), dtype=torch.long, device=device))\n",
        "idx.append(torch.zeros((1, 1), dtype=torch.long, device=device))\n",
        "\n",
        "idx = model.generate(idx, max_new_tokens=500)\n",
        "s=''\n",
        "for t in idx[0][0].tolist():\n",
        "  s+=str(w.decode_dict.get(t))\n",
        "\n",
        "print(s)\n",
        "\n",
        "w.log[\"sample_output\"] = s\n",
        "\n",
        "w.log[\"model_end\"] = (datetime.datetime.now().year,\n",
        "                            datetime.datetime.now().month,\n",
        "                            datetime.datetime.now().day,\n",
        "                            datetime.datetime.now().hour,\n",
        "                            datetime.datetime.now().minute)\n",
        "\n",
        "if _save_needed: save_dict_to_csv_gdrive(w.log, 'log_data.csv')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V5E1",
      "provenance": [],
      "authorship_tag": "ABX9TyNrslfOj+AVJP0vUJWmpCXE",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}