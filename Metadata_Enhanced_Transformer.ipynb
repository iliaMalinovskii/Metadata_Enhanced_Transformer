{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iliaMalinovskii/Metadata_Enhanced_Transformer/blob/main/Metadata_Enhanced_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rnc8KAdyM2mV",
        "outputId": "db637f77-ba65-4c50-ecbf-ba42d3dd67db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        }
      ],
      "source": [
        "from operator import pos\n",
        "# @title Default title text\n",
        "\n",
        "import copy\n",
        "import random\n",
        "import itertools\n",
        "\n",
        "!pip install nltk\n",
        "import nltk\n",
        "import os\n",
        "from google.colab import drive\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "import spacy\n",
        "\n",
        "import datetime\n",
        "import csv\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "#...\n",
        "\n",
        "nltk.download('averaged_perceptron_tagger', download_dir='/root/nltk_data')\n",
        "nltk.data.path.append('/root/nltk_data') # Tell nltk to include the new directory in the search path\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize, RegexpTokenizer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "import re\n",
        "\n",
        "#...\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class Workflow:\n",
        "  def __init__(self):\n",
        "    self.log = {}\n",
        "    self.alltext =\"\"\n",
        "    self.decode_dict={}\n",
        "    self.pos_list =[]\n",
        "    self.list_of_seq = []\n",
        "    self.data = []\n",
        "    self.train_data = []\n",
        "    self.val_data = []\n",
        "    self.vocab_sizes = []\n",
        "    return\n",
        "\n",
        "  def load_texts(self, max_text = 5, base_tokens_dict = None,\n",
        "                 drive_mount_path = '/content/drive',\n",
        "                 folder_path = '/content/drive/MyDrive/Colab Notebooks/fairy_tales'):\n",
        "\n",
        "    drive.mount(drive_mount_path)\n",
        "    text_count = 0\n",
        "    base_token_seq = []\n",
        "    for filename in os.listdir(folder_path):\n",
        "      if filename.endswith(\".txt\"):\n",
        "        filepath = os.path.join(folder_path, filename)\n",
        "        try:\n",
        "          with open(filepath, 'r', encoding='utf-8',errors='ignore') as f:\n",
        "            file_text = f.read().lower() # TODO: remove first row, that is title\n",
        "            file_text = ' '.join(file_text.strip().split())\n",
        "            file_text = re.sub(r'[^a-zA-Z0-9\\s]', '', file_text)\n",
        "            file_text = re.sub(r'\\s+', ' ', file_text).strip()\n",
        "            #file_text = re.sub(r\"\\s+\", \" \", file_text).strip()\n",
        "            if text_count >0: file_text = \" \"+file_text\n",
        "            prev_t = None\n",
        "            if text_count >0: prev_t = base_token_seq[-1]\n",
        "            for t in file_text:\n",
        "              token = base_tokens_dict.get(t)\n",
        "              if token == None: continue\n",
        "              if t == \" \" and prev_t == \" \": continue\n",
        "              base_token_seq.append(t)\n",
        "              prev_t = t\n",
        "            text_count += 1\n",
        "            if (100*(text_count/max_text)) % 20 == 0: print(f\"{text_count} texts loaded\")\n",
        "            if text_count >= max_text: break\n",
        "        except Exception as e:\n",
        "          print(f\"Error reading file {filename}: {e}\")\n",
        "\n",
        "    self.alltext = ''.join(base_token_seq)\n",
        "    #print(\"here 0.1\", base_token_seq)\n",
        "    #print(\"here 0.2\", len(base_token_seq))\n",
        "    #print(\"here 0.3\", base_token_seq[-1]==\" \")\n",
        "\n",
        "    return base_token_seq\n",
        "\n",
        "  def dictionaries(self):\n",
        "    list_of_dicts = {}\n",
        "\n",
        "    # Dictionary #1 MAIN\n",
        "    alphabet_value = 0\n",
        "    alphabet_dict = {}\n",
        "    alphabet = 'abcdefghijklmnopqrstuvwxyz0123456789 '\n",
        "    for t in alphabet:\n",
        "      alphabet_dict[t] = alphabet_value\n",
        "      self.decode_dict[alphabet_value] = t\n",
        "      alphabet_value += 1\n",
        "\n",
        "    list_of_dicts[\"MAIN\"] = alphabet_dict\n",
        "    # Dictionary #2\n",
        "    morphemes_value = 1\n",
        "    morphemes_dict = {}\n",
        "    morphemes_list = [\"ab\", \"ad\", \"ante\", \"anti\", \"auto\", \"ation\", \"ative\",\n",
        "                      \"be\", \"bi\", \"circum\", \"co\", \"com\", \"con\", \"counter\",\n",
        "                      \"de\", \"dis\",\"em\", \"en\", \"epi\", \"es\", \"eu\", \"ex\", \"extra\",\n",
        "                      \"hyper\", \"hypo\", \"ible\", \"il\", \"im\", \"in\", \"inter\",\n",
        "                      \"intra\",\"ion\", \"ir\", \"ise\", \"iso\", \"ition\", \"itive\",\n",
        "                      \"mal\",\"mid\", \"mis\", \"mono\", \"non\", \"ob\", \"omni\",\"or\",\n",
        "                      \"out\", \"over\", \"post\", \"pre\", \"pro\", \"re\", \"semi\",\n",
        "                      \"sub\", \"super\", \"trans\", \"ty\", \"un\", \"under\", \"uni\",\n",
        "                      \"vice\", \"ward\", \"with\", \"wise\", \"able\", \"al\", \"ance\",\n",
        "                      \"ant\", \"ary\", \"ate\", \"dom\", \"ed\", \"ence\", \"ency\", \"er\",\n",
        "                      \"est\", \"eous\", \"fore,\" \"ful\", \"fy\", \"hood\", \"ic\", \"ical\",\n",
        "                      \"ial\", \"ify\", \"ing\", \"ious\", \"ism\", \"ist\", \"ity\", \"ive\",\n",
        "                      \"ize\", \"less\", \"ly\", \"ment\", \"ness\", \"ous\", \"ship\",\n",
        "                      \"sion\", \"tion\", \"ure\"]\n",
        "\n",
        "    for m in morphemes_list:\n",
        "      morphemes_dict[m] = morphemes_value\n",
        "      morphemes_value += 1\n",
        "\n",
        "    list_of_dicts[\"MORPH\"] = morphemes_dict\n",
        "    return list_of_dicts\n",
        "\n",
        "  def pos_dict(self, default_value = 0):\n",
        "\n",
        "    # Dictionary #3\n",
        "\n",
        "    self.pos_list = [\"CC\", #coordinating conjunction\n",
        "      \"CD\", #cardinal digit\n",
        "      \"DT\", #determiner\n",
        "      \"EX\", #existential there (like: “there is” … think of it like “there exists”)\n",
        "      \"FW\", #foreign word\n",
        "      \"IN\", #preposition/subordinating conjunction\n",
        "      \"JJ\", # adjective – ‘big’\n",
        "      \"JJR\", # adjective, comparative – ‘bigger’\n",
        "      \"JJS\", # adjective, superlative – ‘biggest’\n",
        "      \"LS\", # list marker 1)\n",
        "      \"MD\", # modal – could, will\n",
        "      \"NN\", # noun, singular ‘- desk’\n",
        "      \"NNS\", # noun plural – ‘desks’\n",
        "      \"NNP\", # proper noun, singular – ‘Harrison’\n",
        "      \"NNPS\", # proper noun, plural – ‘Americans’\n",
        "      \"PDT\", # predeterminer – ‘all the kids’\n",
        "      \"POS\", # possessive ending parent’s\n",
        "      \"PRP\", # personal pronoun –  I, he, she\n",
        "      \"PRP$\", # possessive pronoun – my, his, hers\n",
        "      \"RB\", # adverb – very, silently,\n",
        "      \"RBR\", # adverb, comparative – better\n",
        "      \"RBS\", # adverb, superlative – best\n",
        "      \"RP\", # particle – give up\n",
        "      \"TO\", # – to go ‘to’ the store.\n",
        "      \"UH\", # interjection – errrrrrrrm\n",
        "      \"VB\", # verb, base form – take\n",
        "      \"VBD\", # verb, past tense – took\n",
        "      \"VBG\", # verb, gerund/present participle – taking\n",
        "      \"VBN\", # verb, past participle – taken\n",
        "      \"VBP\", # verb, sing. present, non-3d – take\n",
        "      \"VBZ\", # verb, 3rd person sing. present – takes\n",
        "      \"WDT\", # wh-determiner – which\n",
        "      \"WP\", # wh-pronoun – who, what\n",
        "      \"WP$\", # possessive wh-pronoun, eg- whose\n",
        "      \"WRB\"] # wh-adverb, eg- where, when\n",
        "\n",
        "    pos_dict = {}\n",
        "    pos_dict_value = 1\n",
        "    for p in self.pos_list:\n",
        "      pos_dict[p] = pos_dict_value\n",
        "      pos_dict_value += 1\n",
        "\n",
        "    #print(\"here\",(self.alltext))\n",
        "    #print(\"here 0\",(self.alltext[len(self.alltext)-1]))\n",
        "\n",
        "\n",
        "    tokenized = self.alltext.split()\n",
        "    #print(\"here 1.1\", (tokenized))\n",
        "    #print(\"here 1.2\", len(tokenized))\n",
        "\n",
        "    pos_seq = nltk.pos_tag(tokenized)\n",
        "    #print(\"here 1.3\", (pos_seq))\n",
        "    #print(\"here 1.3\", len(pos_seq))\n",
        "\n",
        "\n",
        "    pos_tokenized = []\n",
        "    j=0\n",
        "    for t in pos_seq:\n",
        "      j+=1\n",
        "      pos_tag_value = pos_dict.get(t[1])\n",
        "      if pos_tag_value is None: pos_tag_value = default_value\n",
        "      for i in range(len(t[0])):\n",
        "        pos_tokenized.append(pos_tag_value)\n",
        "      if j < len(pos_seq): pos_tokenized.append(default_value)\n",
        "\n",
        "    #print(\"here 2\", len(pos_tokenized))\n",
        "    return pos_tokenized\n",
        "\n",
        "  def translate(dictionary, tokens, default_value=0):\n",
        "    result = [default_value] * len(tokens)\n",
        "    k=0\n",
        "    for key in sorted(dictionary, key=len):\n",
        "      key_len = len(key)\n",
        "      if k % 50 == 0: print(k) #TODO: track explicitly\n",
        "      k+=1\n",
        "      i=0\n",
        "      while i < len(tokens) - key_len + 1:\n",
        "        if result[i] != default_value:\n",
        "          i+=1\n",
        "          continue\n",
        "        subsequence = ''.join(tokens[i:i + key_len])\n",
        "        if subsequence == key:\n",
        "          for j in range(key_len):\n",
        "            result[i + j] = dictionary[key]\n",
        "          i+=j+1\n",
        "        else: i+=1\n",
        "    return result\n",
        "\n",
        "  def data_prep(self, max_text = 5, default_value = 0, load_seq = None):\n",
        "    n = -1\n",
        "    i = 0\n",
        "\n",
        "    for l in load_seq:\n",
        "      match l:\n",
        "        case \"MAIN\":\n",
        "          self.list_of_dicts = self.dictionaries()\n",
        "          self.base_token_seq = self.load_texts(max_text = max_text, base_tokens_dict = self.list_of_dicts.get(\"MAIN\"))\n",
        "          self.list_of_seq.append(Workflow.translate(self.list_of_dicts.get(\"MAIN\"), self.base_token_seq, default_value=default_value))\n",
        "          self.data.append(torch.tensor(self.list_of_seq[i], dtype=torch.long))\n",
        "          #print(len(self.data[i]))\n",
        "          self.vocab_sizes.append(len(self.list_of_dicts.get(\"MAIN\")))\n",
        "          n = int(share_train*len(self.data[i]))\n",
        "          self.train_data.append(self.data[i][:n])\n",
        "          self.val_data.append(self.data[i][n:])\n",
        "          i+=1\n",
        "        case \"POS\":\n",
        "          pos_seq = self.pos_dict()\n",
        "          #print(len(pos_seq))\n",
        "          self.list_of_seq.append(pos_seq)\n",
        "          pos_vocab_size = len(self.pos_list)+2\n",
        "          if len(load_seq) > 2: pos_vocab_size = max(list(self.list_of_dicts.get(\"MORPH\").values()))+2 #band aid - keep sizes the same\n",
        "          self.vocab_sizes.append(pos_vocab_size)\n",
        "          self.data.append(torch.tensor(self.list_of_seq[i], dtype=torch.long))\n",
        "          self.train_data.append(self.data[i][:n])\n",
        "          self.val_data.append(self.data[i][n:])\n",
        "          i+=1\n",
        "        case \"MORPH\":\n",
        "          self.list_of_seq.append(Workflow.translate(self.list_of_dicts.get(\"MORPH\"),self.base_token_seq,default_value = default_value))\n",
        "          self.data.append(torch.tensor(self.list_of_seq[i], dtype=torch.long))\n",
        "          self.vocab_sizes.append(max(list(self.list_of_dicts.get(\"MORPH\").values()))+2)\n",
        "          self.train_data.append(self.data[i][:n])\n",
        "          self.val_data.append(self.data[i][n:])\n",
        "          i+=1\n",
        "        case \"RND\":\n",
        "          rnd_list = [random.randint(0, 101) for iter in range(len(self.data[0]))]\n",
        "          self.list_of_seq.append(rnd_list)\n",
        "          self.data.append(torch.tensor(self.list_of_seq[i], dtype=torch.long))\n",
        "          self.vocab_sizes.append(102) #hardcoded for comparison\n",
        "          self.train_data.append(self.data[i][:n])\n",
        "          self.val_data.append(self.data[i][n:])\n",
        "          i+=1\n",
        "    return\n",
        "\n",
        "  def get_batch(self, split):\n",
        "    data = self.train_data if split == 'train' else self.val_data\n",
        "    ix = torch.randint(len(data[0]) - block_size-1, (batch_size,))\n",
        "    x = []\n",
        "    y = []\n",
        "    for j in range(len(data)):\n",
        "      x.append(torch.stack([data[j][i : i + block_size] for i in ix]))\n",
        "      y.append(torch.stack([data[j][i + 1 : i + 1 + block_size] for i in ix]))\n",
        "      x[j], y[j] = x[j].to(device), y[j].to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss(model, w):\n",
        "  out = {}\n",
        "  out_detailed = {}\n",
        "  ind_losses = torch.zeros((input_dim, eval_iters))\n",
        "  model.eval()\n",
        "  for split in ['train', 'val']:\n",
        "    losses = torch.zeros(eval_iters)\n",
        "    for k in range(eval_iters):\n",
        "      X, Y = w.get_batch(split)\n",
        "      logits, loss, individual_losses = model(X, Y)\n",
        "      losses[k] = loss.item()\n",
        "      for i in range(input_dim):\n",
        "        ind_losses[i][k] = individual_losses[i].item()\n",
        "    out[split] = losses.mean()\n",
        "    i_losses = []\n",
        "    for i in range(input_dim): i_losses.append(ind_losses[i].mean())\n",
        "    out_detailed[split] = i_losses #(ind_losses[0].mean(),ind_losses[1].mean(),ind_losses[2].mean())\n",
        "  model.train()\n",
        "  return out, out_detailed\n",
        "\n",
        "class Head(nn.Module):\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # input of size (batch, T = block size ... seq length, C=embedding size)\n",
        "        # output of size (batch, time-step, head size)\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x) # (B,T,hs)\n",
        "        q = self.query(x) # (B,T,hs)\n",
        "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        v = self.value(x) # (B,T,hs)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
        "        return out\n",
        "\n",
        "class HeadXAttn(nn.Module):\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, w):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(w)\n",
        "        q = self.query(x) # (B,T,hs)\n",
        "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        v = self.value(w) # (B,T,hs)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class MultiHeadXAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([HeadXAttn(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, w):\n",
        "        out = torch.cat([h(x,w) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, n_embd, n_head, num_frames):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "\n",
        "        self.ln_t1 = nn.LayerNorm(n_embd)\n",
        "        self.sa_t = MultiHeadAttention(n_head, head_size)\n",
        "        self.ln_t2 = nn.LayerNorm(n_embd)\n",
        "        self.ffwd_t = FeedFoward(n_embd)\n",
        "\n",
        "        self.ln_f1 = nn.ModuleList([nn.LayerNorm(n_embd) for _ in range(num_frames)])\n",
        "        self.sa_f = nn.ModuleList([MultiHeadAttention(n_head, head_size) for _ in range(num_frames)])\n",
        "        self.ln_f2 = nn.ModuleList([nn.LayerNorm(n_embd) for _ in range(num_frames)])\n",
        "        self.ffwd_f = nn.ModuleList([FeedFoward(n_embd) for _ in range(num_frames)])\n",
        "\n",
        "        self.ln_x1 = nn.ModuleList([nn.LayerNorm(n_embd) for _ in range(num_frames)])\n",
        "        self.ln_w1 = nn.ModuleList([nn.LayerNorm(n_embd) for _ in range(num_frames)])\n",
        "        self.sa_x = nn.ModuleList([MultiHeadXAttention(n_head, head_size) for _ in range(num_frames)])\n",
        "        self.ln_x2 = nn.ModuleList([nn.LayerNorm(n_embd) for _ in range(num_frames)])\n",
        "        self.ffwd_x = nn.ModuleList([FeedFoward(n_embd) for _ in range(num_frames)])\n",
        "\n",
        "        self.sa_w = nn.ModuleList([MultiHeadXAttention(n_head, head_size) for _ in range(num_frames)])\n",
        "        self.ln_w2 = nn.ModuleList([nn.LayerNorm(n_embd) for _ in range(num_frames)])\n",
        "        self.ffwd_w = nn.ModuleList([FeedFoward(n_embd) for _ in range(num_frames)])\n",
        "\n",
        "    def forward(self, x, w):\n",
        "        x = x + self.sa_t(self.ln_t1(x))\n",
        "        x = x + self.ffwd_t(self.ln_t2(x))\n",
        "        # Stack w into a single tensor\n",
        "        #w = torch.stack(w, dim=0) # (num_frames, B, T, C)\n",
        "\n",
        "        # Apply layers to the stacked w tensor\n",
        "        if w is not None:\n",
        "          w = w + torch.stack([sa_f(ln_f1(w_frame))\n",
        "                            for sa_f, ln_f1, w_frame in zip(self.sa_f, self.ln_f1, w)], dim=0)\n",
        "          w = w + torch.stack([ffwd_f(ln_f2(w_frame))\n",
        "                            for ffwd_f, ln_f2, w_frame in zip(self.ffwd_f, self.ln_f2, w)], dim=0)\n",
        "\n",
        "        # Repeat x along the num_frames dimension\n",
        "        x_shape = x.shape\n",
        "        if w is not None:\n",
        "          x = x.unsqueeze(0).repeat(w.shape[0], 1, 1, 1) # (num_frames, B, T, C)\n",
        "\n",
        "        # Apply cross-attention and feedforward layers\n",
        "        if w is not None:\n",
        "          x = x + torch.stack([sa_x(ln_x1(x_frame), ln_w1(w_frame))\n",
        "                            for sa_x, ln_x1, ln_w1, x_frame, w_frame in zip(self.sa_x, self.ln_x1, self.ln_w1, x, w)], dim=0)\n",
        "          x = x + torch.stack([ffwd_x(ln_x2(x_frame))\n",
        "                            for ffwd_x, ln_x2, x_frame in zip(self.ffwd_x, self.ln_x2, x)], dim=0)\n",
        "\n",
        "        if w is not None:\n",
        "          w = w + torch.stack([sa_w(ln_w1(w_frame), ln_x1(x_frame))\n",
        "                            for sa_w, ln_w1, ln_x1, w_frame, x_frame in zip(self.sa_w, self.ln_w1, self.ln_x1, w, x)], dim=0)\n",
        "          w = w + torch.stack([ffwd_w(ln_w2(w_frame))\n",
        "                            for ffwd_w, ln_w2, w_frame in zip(self.ffwd_w, self.ln_w2, w)], dim=0)\n",
        "\n",
        "          # Average x across the num_frames dimension\n",
        "          x = torch.mean(x, dim=0)\n",
        "\n",
        "        return x, w\n",
        "\n",
        "class GPTLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_sizes):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_sizes[0], n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_sizes[0])\n",
        "\n",
        "        self.frame_embedding_table = []\n",
        "        self.frame_position_emb_table = []\n",
        "        self.ln_w = []\n",
        "        self.lm_head_w = []\n",
        "\n",
        "        self.frame_embedding_table = nn.ModuleList([nn.Embedding(v, n_embd) for v in vocab_sizes[1:]])\n",
        "        self.frame_position_emb_table = nn.ModuleList([nn.Embedding(block_size, n_embd) for _ in range(len(vocab_sizes) - 1)])\n",
        "        self.ln_w = nn.ModuleList([nn.LayerNorm(n_embd) for _ in range(len(vocab_sizes) - 1)])\n",
        "        self.lm_head_w = nn.ModuleList([nn.Linear(n_embd, v) for v in vocab_sizes[1:]])\n",
        "\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head,num_frames=len(vocab_sizes)-1) for _ in range(n_layer)])\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx[0].shape # w is of the same size\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx[0]) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "\n",
        "        if input_dim >1:\n",
        "          w = torch.stack([emb(i) + pos(torch.arange(T, device=device))\n",
        "            for emb, pos, i in zip(self.frame_embedding_table, self.frame_position_emb_table, idx[1:])], dim=0)\n",
        "        else: w = None\n",
        "\n",
        "        for block in self.blocks:\n",
        "          x, w = block(x,w) # (B,T,C)\n",
        "\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if len(idx)>1:\n",
        "          logits_w = torch.stack([lm_head(ln(w_frame))\n",
        "            for lm_head, ln, w_frame in zip(self.lm_head_w, self.ln_w, w)], dim=0)\n",
        "        else: logits_w = None\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets_m = targets[0].view(B*T)\n",
        "            if len(idx)>1:\n",
        "              logits_w_reshaped = [lw.view(B*T, lw.shape[-1]) for lw in logits_w]\n",
        "              target_w = [t.view(B*T) for t in targets[1:]]\n",
        "\n",
        "            individual_losses = [F.cross_entropy(logits, targets_m)]\n",
        "\n",
        "            if len(idx)>1:\n",
        "              individual_losses.extend([F.cross_entropy(lw, tw) for lw, tw in zip(logits_w_reshaped, target_w)])\n",
        "              loss = main_seq_loss_contribution_weight * individual_losses[0] + (1-main_seq_loss_contribution_weight) * sum(individual_losses[1:]) / len(individual_losses[1:])\n",
        "            else:\n",
        "              loss = individual_losses[0]\n",
        "\n",
        "        return (logits,logits_w), loss, individual_losses\n",
        "\n",
        "def save_dict_to_csv_gdrive(dictionary, filename, folder_path='/content/drive/My Drive/Colab Notebooks/'):\n",
        "  drive.mount('/content/drive')\n",
        "  filepath = os.path.join(folder_path, filename)\n",
        "\n",
        "  with open(filepath, 'a', newline='') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    for key, value in dictionary.items():\n",
        "      writer.writerow([key, value])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANdD0OS3M7ju",
        "outputId": "216b276f-b7d1-46d1-ee49-4214fe5e4716"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "330 texts loaded\n",
            "660 texts loaded\n",
            "990 texts loaded\n",
            "1320 texts loaded\n",
            "1650 texts loaded\n",
            "0\n",
            "0.393381 M parameters\n",
            "step 0: train loss 3.6379, val loss 3.6375\n",
            "step 0: train loss 3.6379, val loss 3.6375\n",
            "step 250: train loss 2.3395, val loss 2.3291\n",
            "step 250: train loss 2.3395, val loss 2.3291\n",
            "step 500: train loss 2.1999, val loss 2.1914\n",
            "step 500: train loss 2.1999, val loss 2.1914\n",
            "step 750: train loss 2.1206, val loss 2.1110\n",
            "step 750: train loss 2.1206, val loss 2.1110\n",
            "step 1000: train loss 2.0091, val loss 2.0017\n",
            "step 1000: train loss 2.0091, val loss 2.0017\n",
            "step 1250: train loss 1.8698, val loss 1.8671\n",
            "step 1250: train loss 1.8698, val loss 1.8671\n",
            "step 1500: train loss 1.7590, val loss 1.7679\n",
            "step 1500: train loss 1.7590, val loss 1.7679\n",
            "step 1750: train loss 1.6826, val loss 1.7058\n",
            "step 1750: train loss 1.6826, val loss 1.7058\n",
            "step 2000: train loss 1.6331, val loss 1.6591\n",
            "step 2000: train loss 1.6331, val loss 1.6591\n",
            "step 2250: train loss 1.5919, val loss 1.6319\n",
            "step 2250: train loss 1.5919, val loss 1.6319\n",
            "step 2500: train loss 1.5573, val loss 1.6048\n",
            "step 2500: train loss 1.5573, val loss 1.6048\n",
            "step 2750: train loss 1.5278, val loss 1.5822\n",
            "step 2750: train loss 1.5278, val loss 1.5822\n",
            "step 3000: train loss 1.5016, val loss 1.5693\n",
            "step 3000: train loss 1.5016, val loss 1.5693\n",
            "step 3250: train loss 1.4773, val loss 1.5511\n",
            "step 3250: train loss 1.4773, val loss 1.5511\n",
            "step 3500: train loss 1.4532, val loss 1.5361\n",
            "step 3500: train loss 1.4532, val loss 1.5361\n",
            "step 3750: train loss 1.4366, val loss 1.5200\n",
            "step 3750: train loss 1.4366, val loss 1.5200\n",
            "step 4000: train loss 1.4209, val loss 1.5062\n",
            "step 4000: train loss 1.4209, val loss 1.5062\n",
            "step 4250: train loss 1.4098, val loss 1.5073\n",
            "step 4250: train loss 1.4098, val loss 1.5073\n",
            "step 4500: train loss 1.3877, val loss 1.4922\n",
            "step 4500: train loss 1.3877, val loss 1.4922\n",
            "step 4750: train loss 1.3789, val loss 1.4832\n",
            "step 4750: train loss 1.3789, val loss 1.4832\n",
            "step 4999: train loss 1.3683, val loss 1.4800\n",
            "step 4999: train loss 1.3683, val loss 1.4800\n",
            "{'new_model_start': (2025, 6, 3, 22, 2), 'memo': 'test1 re run smaller train set v2', 'batch_size': 32, 'block_size': 128, 'max_iters': 5000, 'eval_interval': 250, 'learning_rate': 0.0003, 'device': 'cpu', 'eval_iters': 200, 'n_embd': 56, 'n_head': 8, 'n_layer': 10, 'dropout': 0.2, 'max_texts': 1650, 'seq_order': ['MAIN'], 'input_dim': 1, 'main_seq_loss_contribution_weight': 0.8, 'share_train': 0.02, 'vocab_size': [37], 'is timestamp': False, 'limit_time_sec': 0, 'model parameters, M': 0.393381, 'model_training_start': (2025, 6, 3, 22, 5), 'reach_limit_time': False, 'step 0: train loss': 3.6379470825195312, 'step 0: val loss': 3.637516736984253, 'step 0 0: train loss': 3.6379470825195312, 'step 0 0: val loss': 3.637516736984253, 'step 250: train loss': 2.3394994735717773, 'step 250: val loss': 2.329087257385254, 'step 250 0: train loss': 2.3394994735717773, 'step 250 0: val loss': 2.329087257385254, 'step 500: train loss': 2.1999268531799316, 'step 500: val loss': 2.191351890563965, 'step 500 0: train loss': 2.1999268531799316, 'step 500 0: val loss': 2.191351890563965, 'step 750: train loss': 2.1206209659576416, 'step 750: val loss': 2.1109728813171387, 'step 750 0: train loss': 2.1206209659576416, 'step 750 0: val loss': 2.1109728813171387, 'step 1000: train loss': 2.0091280937194824, 'step 1000: val loss': 2.001676082611084, 'step 1000 0: train loss': 2.0091280937194824, 'step 1000 0: val loss': 2.001676082611084, 'step 1250: train loss': 1.8698176145553589, 'step 1250: val loss': 1.867108941078186, 'step 1250 0: train loss': 1.8698176145553589, 'step 1250 0: val loss': 1.867108941078186, 'step 1500: train loss': 1.7589895725250244, 'step 1500: val loss': 1.7678600549697876, 'step 1500 0: train loss': 1.7589895725250244, 'step 1500 0: val loss': 1.7678600549697876, 'step 1750: train loss': 1.6826002597808838, 'step 1750: val loss': 1.7057684659957886, 'step 1750 0: train loss': 1.6826002597808838, 'step 1750 0: val loss': 1.7057684659957886, 'step 2000: train loss': 1.6331416368484497, 'step 2000: val loss': 1.6590975522994995, 'step 2000 0: train loss': 1.6331416368484497, 'step 2000 0: val loss': 1.6590975522994995, 'step 2250: train loss': 1.5919266939163208, 'step 2250: val loss': 1.6318957805633545, 'step 2250 0: train loss': 1.5919266939163208, 'step 2250 0: val loss': 1.6318957805633545, 'step 2500: train loss': 1.5572772026062012, 'step 2500: val loss': 1.6048483848571777, 'step 2500 0: train loss': 1.5572772026062012, 'step 2500 0: val loss': 1.6048483848571777, 'step 2750: train loss': 1.5278403759002686, 'step 2750: val loss': 1.582217812538147, 'step 2750 0: train loss': 1.5278403759002686, 'step 2750 0: val loss': 1.582217812538147, 'step 3000: train loss': 1.501627802848816, 'step 3000: val loss': 1.5692570209503174, 'step 3000 0: train loss': 1.501627802848816, 'step 3000 0: val loss': 1.5692570209503174, 'step 3250: train loss': 1.477262258529663, 'step 3250: val loss': 1.5510848760604858, 'step 3250 0: train loss': 1.477262258529663, 'step 3250 0: val loss': 1.5510848760604858, 'step 3500: train loss': 1.453197956085205, 'step 3500: val loss': 1.5360854864120483, 'step 3500 0: train loss': 1.453197956085205, 'step 3500 0: val loss': 1.5360854864120483, 'step 3750: train loss': 1.436631441116333, 'step 3750: val loss': 1.5199655294418335, 'step 3750 0: train loss': 1.436631441116333, 'step 3750 0: val loss': 1.5199655294418335, 'step 4000: train loss': 1.4208579063415527, 'step 4000: val loss': 1.5061911344528198, 'step 4000 0: train loss': 1.4208579063415527, 'step 4000 0: val loss': 1.5061911344528198, 'step 4250: train loss': 1.4098361730575562, 'step 4250: val loss': 1.5073283910751343, 'step 4250 0: train loss': 1.4098361730575562, 'step 4250 0: val loss': 1.5073283910751343, 'step 4500: train loss': 1.3877441883087158, 'step 4500: val loss': 1.4921907186508179, 'step 4500 0: train loss': 1.3877441883087158, 'step 4500 0: val loss': 1.4921907186508179, 'step 4750: train loss': 1.3788745403289795, 'step 4750: val loss': 1.483244776725769, 'step 4750 0: train loss': 1.3788745403289795, 'step 4750 0: val loss': 1.483244776725769, 'step 4999: train loss': 1.3682767152786255, 'step 4999: val loss': 1.480037808418274, 'step 4999 0: train loss': 1.3682767152786255, 'step 4999 0: val loss': 1.480037808418274, 'model_end': (2025, 6, 3, 22, 46)}\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "330 texts loaded\n",
            "660 texts loaded\n",
            "990 texts loaded\n",
            "1320 texts loaded\n",
            "1650 texts loaded\n",
            "0\n",
            "0\n",
            "50\n",
            "0.370314 M parameters\n",
            "step 0: train loss 3.8085, val loss 3.8082\n",
            "step 0: train loss 3.5927, val loss 3.5922\n",
            "step 0: train loss 4.6719, val loss 4.6720\n",
            "step 250: train loss 2.2644, val loss 2.2577\n",
            "step 250: train loss 2.4839, val loss 2.4769\n",
            "step 250: train loss 1.3864, val loss 1.3813\n",
            "step 500: train loss 1.9694, val loss 1.9634\n",
            "step 500: train loss 2.2072, val loss 2.2018\n",
            "step 500: train loss 1.0184, val loss 1.0100\n",
            "step 750: train loss 1.7443, val loss 1.7389\n",
            "step 750: train loss 1.9663, val loss 1.9604\n",
            "step 750: train loss 0.8561, val loss 0.8530\n",
            "step 1000: train loss 1.6321, val loss 1.6301\n",
            "step 1000: train loss 1.8460, val loss 1.8438\n",
            "step 1000: train loss 0.7768, val loss 0.7751\n",
            "step 1250: train loss 1.5464, val loss 1.5466\n",
            "step 1250: train loss 1.7504, val loss 1.7516\n",
            "step 1250: train loss 0.7303, val loss 0.7265\n",
            "step 1500: train loss 1.4950, val loss 1.4934\n",
            "step 1500: train loss 1.6937, val loss 1.6923\n",
            "step 1500: train loss 0.6999, val loss 0.6982\n",
            "step 1750: train loss 1.4381, val loss 1.4380\n",
            "step 1750: train loss 1.6321, val loss 1.6316\n",
            "step 1750: train loss 0.6619, val loss 0.6639\n",
            "step 2000: train loss 1.3817, val loss 1.3867\n",
            "step 2000: train loss 1.5691, val loss 1.5757\n",
            "step 2000: train loss 0.6322, val loss 0.6307\n",
            "step 2250: train loss 1.3387, val loss 1.3482\n",
            "step 2250: train loss 1.5231, val loss 1.5335\n",
            "step 2250: train loss 0.6010, val loss 0.6068\n",
            "step 2500: train loss 1.3062, val loss 1.3160\n",
            "step 2500: train loss 1.4874, val loss 1.4983\n",
            "step 2500: train loss 0.5816, val loss 0.5868\n",
            "step 2750: train loss 1.2723, val loss 1.2882\n",
            "step 2750: train loss 1.4498, val loss 1.4671\n",
            "step 2750: train loss 0.5624, val loss 0.5724\n",
            "step 3000: train loss 1.2514, val loss 1.2694\n",
            "step 3000: train loss 1.4272, val loss 1.4471\n",
            "step 3000: train loss 0.5483, val loss 0.5588\n",
            "step 3250: train loss 1.2305, val loss 1.2496\n",
            "step 3250: train loss 1.4035, val loss 1.4250\n",
            "step 3250: train loss 0.5385, val loss 0.5479\n",
            "step 3500: train loss 1.2142, val loss 1.2386\n",
            "step 3500: train loss 1.3849, val loss 1.4130\n",
            "step 3500: train loss 0.5315, val loss 0.5409\n",
            "step 3750: train loss 1.1999, val loss 1.2245\n",
            "step 3750: train loss 1.3703, val loss 1.3984\n",
            "step 3750: train loss 0.5181, val loss 0.5289\n",
            "step 4000: train loss 1.1811, val loss 1.2118\n",
            "step 4000: train loss 1.3486, val loss 1.3836\n",
            "step 4000: train loss 0.5109, val loss 0.5245\n",
            "step 4250: train loss 1.1678, val loss 1.2017\n",
            "step 4250: train loss 1.3342, val loss 1.3735\n",
            "step 4250: train loss 0.5020, val loss 0.5146\n",
            "step 4500: train loss 1.1565, val loss 1.1956\n",
            "step 4500: train loss 1.3218, val loss 1.3666\n",
            "step 4500: train loss 0.4954, val loss 0.5114\n",
            "step 4750: train loss 1.1440, val loss 1.1866\n",
            "step 4750: train loss 1.3073, val loss 1.3568\n",
            "step 4750: train loss 0.4907, val loss 0.5059\n",
            "step 4999: train loss 1.1380, val loss 1.1799\n",
            "step 4999: train loss 1.3012, val loss 1.3492\n",
            "step 4999: train loss 0.4853, val loss 0.5028\n",
            "{'new_model_start': (2025, 6, 3, 22, 48), 'memo': 'test2 re run smaller train set v2', 'batch_size': 32, 'block_size': 128, 'max_iters': 5000, 'eval_interval': 250, 'learning_rate': 0.0003, 'device': 'cpu', 'eval_iters': 200, 'n_embd': 32, 'n_head': 8, 'n_layer': 7, 'dropout': 0.2, 'max_texts': 1650, 'seq_order': ['MAIN', 'MORPH'], 'input_dim': 2, 'main_seq_loss_contribution_weight': 0.8, 'share_train': 0.02, 'vocab_size': [37, 101], 'is timestamp': False, 'limit_time_sec': 0, 'model parameters, M': 0.370314, 'model_training_start': (2025, 6, 3, 22, 56), 'reach_limit_time': False, 'step 0: train loss': 3.808529853820801, 'step 0: val loss': 3.8081624507904053, 'step 0 0: train loss': 3.592682123184204, 'step 0 0: val loss': 3.592200994491577, 'step 0 1: train loss': 4.671920299530029, 'step 0 1: val loss': 4.6720099449157715, 'step 250: train loss': 2.2644076347351074, 'step 250: val loss': 2.257741928100586, 'step 250 0: train loss': 2.4839181900024414, 'step 250 0: val loss': 2.4768526554107666, 'step 250 1: train loss': 1.3863641023635864, 'step 250 1: val loss': 1.3812991380691528, 'step 500: train loss': 1.9694039821624756, 'step 500: val loss': 1.9633995294570923, 'step 500 0: train loss': 2.207162380218506, 'step 500 0: val loss': 2.201752185821533, 'step 500 1: train loss': 1.0183712244033813, 'step 500 1: val loss': 1.0099884271621704, 'step 750: train loss': 1.7442702054977417, 'step 750: val loss': 1.7389187812805176, 'step 750 0: train loss': 1.9663152694702148, 'step 750 0: val loss': 1.9604010581970215, 'step 750 1: train loss': 0.8560897707939148, 'step 750 1: val loss': 0.852989912033081, 'step 1000: train loss': 1.6321285963058472, 'step 1000: val loss': 1.6300948858261108, 'step 1000 0: train loss': 1.845971703529358, 'step 1000 0: val loss': 1.8438327312469482, 'step 1000 1: train loss': 0.7767559885978699, 'step 1000 1: val loss': 0.7751434445381165, 'step 1250: train loss': 1.5463563203811646, 'step 1250: val loss': 1.5465664863586426, 'step 1250 0: train loss': 1.750378131866455, 'step 1250 0: val loss': 1.7515934705734253, 'step 1250 1: train loss': 0.7302692532539368, 'step 1250 1: val loss': 0.7264591455459595, 'step 1500: train loss': 1.4949510097503662, 'step 1500: val loss': 1.4934474229812622, 'step 1500 0: train loss': 1.6937079429626465, 'step 1500 0: val loss': 1.692252516746521, 'step 1500 1: train loss': 0.6999232769012451, 'step 1500 1: val loss': 0.6982275247573853, 'step 1750: train loss': 1.438069462776184, 'step 1750: val loss': 1.4380384683609009, 'step 1750 0: train loss': 1.6321051120758057, 'step 1750 0: val loss': 1.6315772533416748, 'step 1750 1: train loss': 0.6619271039962769, 'step 1750 1: val loss': 0.66388338804245, 'step 2000: train loss': 1.3817131519317627, 'step 2000: val loss': 1.3867045640945435, 'step 2000 0: train loss': 1.5691027641296387, 'step 2000 0: val loss': 1.5757042169570923, 'step 2000 1: train loss': 0.6321538686752319, 'step 2000 1: val loss': 0.6307056546211243, 'step 2250: train loss': 1.3386808633804321, 'step 2250: val loss': 1.3481717109680176, 'step 2250 0: train loss': 1.5231008529663086, 'step 2250 0: val loss': 1.5335191488265991, 'step 2250 1: train loss': 0.6009999513626099, 'step 2250 1: val loss': 0.6067818999290466, 'step 2500: train loss': 1.3062255382537842, 'step 2500: val loss': 1.3159817457199097, 'step 2500 0: train loss': 1.4873777627944946, 'step 2500 0: val loss': 1.4982789754867554, 'step 2500 1: train loss': 0.581616997718811, 'step 2500 1: val loss': 0.5867927670478821, 'step 2750: train loss': 1.2723296880722046, 'step 2750: val loss': 1.2881790399551392, 'step 2750 0: train loss': 1.4498205184936523, 'step 2750 0: val loss': 1.46712327003479, 'step 2750 1: train loss': 0.5623663067817688, 'step 2750 1: val loss': 0.5724016427993774, 'step 3000: train loss': 1.2514351606369019, 'step 3000: val loss': 1.2694240808486938, 'step 3000 0: train loss': 1.4272297620773315, 'step 3000 0: val loss': 1.4470922946929932, 'step 3000 1: train loss': 0.5482571125030518, 'step 3000 1: val loss': 0.558750331401825, 'step 3250: train loss': 1.2304645776748657, 'step 3250: val loss': 1.2495957612991333, 'step 3250 0: train loss': 1.403451681137085, 'step 3250 0: val loss': 1.425026297569275, 'step 3250 1: train loss': 0.5385165810585022, 'step 3250 1: val loss': 0.5478735566139221, 'step 3500: train loss': 1.2142000198364258, 'step 3500: val loss': 1.2386085987091064, 'step 3500 0: train loss': 1.384883165359497, 'step 3500 0: val loss': 1.4130247831344604, 'step 3500 1: train loss': 0.5314674377441406, 'step 3500 1: val loss': 0.5409436821937561, 'step 3750: train loss': 1.199881911277771, 'step 3750: val loss': 1.2244712114334106, 'step 3750 0: train loss': 1.3703281879425049, 'step 3750 0: val loss': 1.3983683586120605, 'step 3750 1: train loss': 0.5180962085723877, 'step 3750 1: val loss': 0.5288822650909424, 'step 4000: train loss': 1.1810705661773682, 'step 4000: val loss': 1.211796522140503, 'step 4000 0: train loss': 1.3486075401306152, 'step 4000 0: val loss': 1.3836114406585693, 'step 4000 1: train loss': 0.5109231472015381, 'step 4000 1: val loss': 0.5245369672775269, 'step 4250: train loss': 1.1677550077438354, 'step 4250: val loss': 1.2017319202423096, 'step 4250 0: train loss': 1.33418607711792, 'step 4250 0: val loss': 1.3735225200653076, 'step 4250 1: train loss': 0.5020300149917603, 'step 4250 1: val loss': 0.514569878578186, 'step 4500: train loss': 1.1565253734588623, 'step 4500: val loss': 1.1956063508987427, 'step 4500 0: train loss': 1.3217943906784058, 'step 4500 0: val loss': 1.3666489124298096, 'step 4500 1: train loss': 0.4954499900341034, 'step 4500 1: val loss': 0.5114361047744751, 'step 4750: train loss': 1.1439820528030396, 'step 4750: val loss': 1.1866087913513184, 'step 4750 0: train loss': 1.3073123693466187, 'step 4750 0: val loss': 1.3567761182785034, 'step 4750 1: train loss': 0.49066054821014404, 'step 4750 1: val loss': 0.5059389472007751, 'step 4999: train loss': 1.1380480527877808, 'step 4999: val loss': 1.1799131631851196, 'step 4999 0: train loss': 1.3012257814407349, 'step 4999 0: val loss': 1.349195122718811, 'step 4999 1: train loss': 0.48533689975738525, 'step 4999 1: val loss': 0.5027858018875122, 'model_end': (2025, 6, 4, 0, 17)}\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "330 texts loaded\n",
            "660 texts loaded\n",
            "990 texts loaded\n",
            "1320 texts loaded\n",
            "1650 texts loaded\n",
            "0\n",
            "0.366154 M parameters\n",
            "step 0: train loss 3.6146, val loss 3.6146\n",
            "step 0: train loss 3.6037, val loss 3.6040\n",
            "step 0: train loss 3.6580, val loss 3.6570\n",
            "step 250: train loss 2.2988, val loss 2.2982\n",
            "step 250: train loss 2.4541, val loss 2.4482\n",
            "step 250: train loss 1.6777, val loss 1.6980\n",
            "step 500: train loss 1.9600, val loss 1.9582\n",
            "step 500: train loss 2.1826, val loss 2.1766\n",
            "step 500: train loss 1.0697, val loss 1.0846\n",
            "step 750: train loss 1.7641, val loss 1.7640\n",
            "step 750: train loss 1.9892, val loss 1.9852\n",
            "step 750: train loss 0.8638, val loss 0.8793\n",
            "step 1000: train loss 1.6139, val loss 1.6074\n",
            "step 1000: train loss 1.8258, val loss 1.8149\n",
            "step 1000: train loss 0.7663, val loss 0.7775\n",
            "step 1250: train loss 1.5305, val loss 1.5252\n",
            "step 1250: train loss 1.7355, val loss 1.7266\n",
            "step 1250: train loss 0.7103, val loss 0.7196\n",
            "step 1500: train loss 1.4764, val loss 1.4725\n",
            "step 1500: train loss 1.6755, val loss 1.6685\n",
            "step 1500: train loss 0.6797, val loss 0.6885\n",
            "step 1750: train loss 1.4309, val loss 1.4312\n",
            "step 1750: train loss 1.6268, val loss 1.6239\n",
            "step 1750: train loss 0.6475, val loss 0.6601\n",
            "step 2000: train loss 1.3954, val loss 1.3933\n",
            "step 2000: train loss 1.5870, val loss 1.5816\n",
            "step 2000: train loss 0.6291, val loss 0.6401\n",
            "step 2250: train loss 1.3608, val loss 1.3567\n",
            "step 2250: train loss 1.5486, val loss 1.5406\n",
            "step 2250: train loss 0.6098, val loss 0.6211\n",
            "step 2500: train loss 1.3246, val loss 1.3294\n",
            "step 2500: train loss 1.5076, val loss 1.5105\n",
            "step 2500: train loss 0.5926, val loss 0.6050\n",
            "step 2750: train loss 1.2960, val loss 1.2975\n",
            "step 2750: train loss 1.4745, val loss 1.4739\n",
            "step 2750: train loss 0.5823, val loss 0.5919\n",
            "step 3000: train loss 1.2631, val loss 1.2797\n",
            "step 3000: train loss 1.4373, val loss 1.4546\n",
            "step 3000: train loss 0.5663, val loss 0.5799\n",
            "step 3250: train loss 1.2388, val loss 1.2567\n",
            "step 3250: train loss 1.4091, val loss 1.4286\n",
            "step 3250: train loss 0.5575, val loss 0.5692\n",
            "step 3500: train loss 1.2218, val loss 1.2384\n",
            "step 3500: train loss 1.3897, val loss 1.4076\n",
            "step 3500: train loss 0.5504, val loss 0.5615\n",
            "step 3750: train loss 1.2021, val loss 1.2174\n",
            "step 3750: train loss 1.3676, val loss 1.3837\n",
            "step 3750: train loss 0.5401, val loss 0.5519\n",
            "step 4000: train loss 1.1841, val loss 1.2049\n",
            "step 4000: train loss 1.3475, val loss 1.3708\n",
            "step 4000: train loss 0.5302, val loss 0.5414\n",
            "step 4250: train loss 1.1665, val loss 1.1897\n",
            "step 4250: train loss 1.3270, val loss 1.3526\n",
            "step 4250: train loss 0.5248, val loss 0.5381\n",
            "step 4500: train loss 1.1479, val loss 1.1802\n",
            "step 4500: train loss 1.3061, val loss 1.3427\n",
            "step 4500: train loss 0.5155, val loss 0.5303\n",
            "step 4750: train loss 1.1354, val loss 1.1699\n",
            "step 4750: train loss 1.2917, val loss 1.3309\n",
            "step 4750: train loss 0.5104, val loss 0.5257\n",
            "step 4999: train loss 1.1221, val loss 1.1571\n",
            "step 4999: train loss 1.2761, val loss 1.3163\n",
            "step 4999: train loss 0.5062, val loss 0.5201\n",
            "{'new_model_start': (2025, 6, 4, 0, 19), 'memo': 'test3 re run smaller train set v2', 'batch_size': 32, 'block_size': 128, 'max_iters': 5000, 'eval_interval': 250, 'learning_rate': 0.0003, 'device': 'cpu', 'eval_iters': 200, 'n_embd': 32, 'n_head': 8, 'n_layer': 7, 'dropout': 0.2, 'max_texts': 1650, 'seq_order': ['MAIN', 'POS'], 'input_dim': 2, 'main_seq_loss_contribution_weight': 0.8, 'share_train': 0.02, 'vocab_size': [37, 37], 'is timestamp': False, 'limit_time_sec': 0, 'model parameters, M': 0.366154, 'model_training_start': (2025, 6, 4, 0, 23), 'reach_limit_time': False, 'step 0: train loss': 3.6145663261413574, 'step 0: val loss': 3.614609718322754, 'step 0 0: train loss': 3.603696346282959, 'step 0 0: val loss': 3.6040163040161133, 'step 0 1: train loss': 3.6580443382263184, 'step 0 1: val loss': 3.6569836139678955, 'step 250: train loss': 2.2988197803497314, 'step 250: val loss': 2.298177480697632, 'step 250 0: train loss': 2.454108953475952, 'step 250 0: val loss': 2.4482202529907227, 'step 250 1: train loss': 1.677662968635559, 'step 250 1: val loss': 1.698006272315979, 'step 500: train loss': 1.959989070892334, 'step 500: val loss': 1.9581729173660278, 'step 500 0: train loss': 2.1825690269470215, 'step 500 0: val loss': 2.176568031311035, 'step 500 1: train loss': 1.0696687698364258, 'step 500 1: val loss': 1.084591031074524, 'step 750: train loss': 1.7641243934631348, 'step 750: val loss': 1.764005422592163, 'step 750 0: train loss': 1.9891974925994873, 'step 750 0: val loss': 1.9851746559143066, 'step 750 1: train loss': 0.8638330101966858, 'step 750 1: val loss': 0.8793277144432068, 'step 1000: train loss': 1.6139311790466309, 'step 1000: val loss': 1.607431173324585, 'step 1000 0: train loss': 1.8258439302444458, 'step 1000 0: val loss': 1.8149135112762451, 'step 1000 1: train loss': 0.7662792801856995, 'step 1000 1: val loss': 0.7775014638900757, 'step 1250: train loss': 1.5304992198944092, 'step 1250: val loss': 1.525195598602295, 'step 1250 0: train loss': 1.7355389595031738, 'step 1250 0: val loss': 1.7265944480895996, 'step 1250 1: train loss': 0.7103399634361267, 'step 1250 1: val loss': 0.7195996642112732, 'step 1500: train loss': 1.4763737916946411, 'step 1500: val loss': 1.4724847078323364, 'step 1500 0: train loss': 1.6755496263504028, 'step 1500 0: val loss': 1.6684885025024414, 'step 1500 1: train loss': 0.6796703934669495, 'step 1500 1: val loss': 0.688469648361206, 'step 1750: train loss': 1.4309206008911133, 'step 1750: val loss': 1.4311778545379639, 'step 1750 0: train loss': 1.6267828941345215, 'step 1750 0: val loss': 1.623944878578186, 'step 1750 1: train loss': 0.6474719047546387, 'step 1750 1: val loss': 0.6601096391677856, 'step 2000: train loss': 1.3953940868377686, 'step 2000: val loss': 1.3933025598526, 'step 2000 0: train loss': 1.5869696140289307, 'step 2000 0: val loss': 1.5815908908843994, 'step 2000 1: train loss': 0.6290931105613708, 'step 2000 1: val loss': 0.6401486992835999, 'step 2250: train loss': 1.3608391284942627, 'step 2250: val loss': 1.3567373752593994, 'step 2250 0: train loss': 1.5485882759094238, 'step 2250 0: val loss': 1.5406421422958374, 'step 2250 1: train loss': 0.6098417639732361, 'step 2250 1: val loss': 0.6211183071136475, 'step 2500: train loss': 1.324580430984497, 'step 2500: val loss': 1.3294109106063843, 'step 2500 0: train loss': 1.5075870752334595, 'step 2500 0: val loss': 1.5105171203613281, 'step 2500 1: train loss': 0.5925542712211609, 'step 2500 1: val loss': 0.6049863696098328, 'step 2750: train loss': 1.2960257530212402, 'step 2750: val loss': 1.2975029945373535, 'step 2750 0: train loss': 1.4744524955749512, 'step 2750 0: val loss': 1.4738997220993042, 'step 2750 1: train loss': 0.5823181867599487, 'step 2750 1: val loss': 0.5919166803359985, 'step 3000: train loss': 1.2631118297576904, 'step 3000: val loss': 1.2796945571899414, 'step 3000 0: train loss': 1.437319040298462, 'step 3000 0: val loss': 1.454649806022644, 'step 3000 1: train loss': 0.5662828683853149, 'step 3000 1: val loss': 0.57987380027771, 'step 3250: train loss': 1.2387844324111938, 'step 3250: val loss': 1.2566783428192139, 'step 3250 0: train loss': 1.409110426902771, 'step 3250 0: val loss': 1.4285545349121094, 'step 3250 1: train loss': 0.55748051404953, 'step 3250 1: val loss': 0.5691730976104736, 'step 3500: train loss': 1.2218012809753418, 'step 3500: val loss': 1.2383689880371094, 'step 3500 0: train loss': 1.3896567821502686, 'step 3500 0: val loss': 1.4075846672058105, 'step 3500 1: train loss': 0.5503785610198975, 'step 3500 1: val loss': 0.5615063309669495, 'step 3750: train loss': 1.2020570039749146, 'step 3750: val loss': 1.2173534631729126, 'step 3750 0: train loss': 1.3675549030303955, 'step 3750 0: val loss': 1.3837180137634277, 'step 3750 1: train loss': 0.5400654077529907, 'step 3750 1: val loss': 0.5518965721130371, 'step 4000: train loss': 1.1840683221817017, 'step 4000: val loss': 1.2048968076705933, 'step 4000 0: train loss': 1.3475462198257446, 'step 4000 0: val loss': 1.3707687854766846, 'step 4000 1: train loss': 0.5301564931869507, 'step 4000 1: val loss': 0.5414090156555176, 'step 4250: train loss': 1.166548490524292, 'step 4250: val loss': 1.1896679401397705, 'step 4250 0: train loss': 1.3269768953323364, 'step 4250 0: val loss': 1.3525675535202026, 'step 4250 1: train loss': 0.5248345136642456, 'step 4250 1: val loss': 0.5380690097808838, 'step 4500: train loss': 1.1479367017745972, 'step 4500: val loss': 1.1802431344985962, 'step 4500 0: train loss': 1.3060510158538818, 'step 4500 0: val loss': 1.342727780342102, 'step 4500 1: train loss': 0.5154790282249451, 'step 4500 1: val loss': 0.5303041338920593, 'step 4750: train loss': 1.1354291439056396, 'step 4750: val loss': 1.1698912382125854, 'step 4750 0: train loss': 1.291682243347168, 'step 4750 0: val loss': 1.3309299945831299, 'step 4750 1: train loss': 0.5104165077209473, 'step 4750 1: val loss': 0.5257360935211182, 'step 4999: train loss': 1.1221195459365845, 'step 4999: val loss': 1.1570700407028198, 'step 4999 0: train loss': 1.2760924100875854, 'step 4999 0: val loss': 1.3163241147994995, 'step 4999 1: train loss': 0.5062288045883179, 'step 4999 1: val loss': 0.5200525522232056, 'model_end': (2025, 6, 4, 1, 43)}\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "330 texts loaded\n",
            "660 texts loaded\n",
            "990 texts loaded\n",
            "1320 texts loaded\n",
            "1650 texts loaded\n",
            "0\n",
            "0\n",
            "50\n",
            "0.381039 M parameters\n",
            "step 0: train loss 3.8229, val loss 3.8227\n",
            "step 0: train loss 3.6246, val loss 3.6242\n",
            "step 0: train loss 4.6250, val loss 4.6249\n",
            "step 0: train loss 4.6072, val loss 4.6079\n",
            "step 250: train loss 2.3173, val loss 2.3134\n",
            "step 250: train loss 2.4741, val loss 2.4670\n",
            "step 250: train loss 1.3458, val loss 1.3437\n",
            "step 250: train loss 2.0350, val loss 2.0538\n",
            "step 500: train loss 1.9408, val loss 1.9404\n",
            "step 500: train loss 2.1417, val loss 2.1390\n",
            "step 500: train loss 1.0175, val loss 1.0164\n",
            "step 500: train loss 1.2561, val loss 1.2750\n",
            "step 750: train loss 1.6030, val loss 1.5974\n",
            "step 750: train loss 1.7732, val loss 1.7660\n",
            "step 750: train loss 0.9020, val loss 0.8934\n",
            "step 750: train loss 0.9423, val loss 0.9526\n",
            "step 1000: train loss 1.4215, val loss 1.4212\n",
            "step 1000: train loss 1.5729, val loss 1.5713\n",
            "step 1000: train loss 0.8166, val loss 0.8124\n",
            "step 1000: train loss 0.8154, val loss 0.8294\n",
            "step 1250: train loss 1.3270, val loss 1.3249\n",
            "step 1250: train loss 1.4693, val loss 1.4665\n",
            "step 1250: train loss 0.7670, val loss 0.7577\n",
            "step 1250: train loss 0.7483, val loss 0.7598\n",
            "step 1500: train loss 1.2694, val loss 1.2692\n",
            "step 1500: train loss 1.4054, val loss 1.4041\n",
            "step 1500: train loss 0.7375, val loss 0.7347\n",
            "step 1500: train loss 0.7132, val loss 0.7252\n",
            "step 1750: train loss 1.2333, val loss 1.2382\n",
            "step 1750: train loss 1.3671, val loss 1.3717\n",
            "step 1750: train loss 0.7136, val loss 0.7142\n",
            "step 1750: train loss 0.6826, val loss 0.6942\n",
            "step 2000: train loss 1.2050, val loss 1.2057\n",
            "step 2000: train loss 1.3361, val loss 1.3356\n",
            "step 2000: train loss 0.6941, val loss 0.6948\n",
            "step 2000: train loss 0.6668, val loss 0.6777\n",
            "step 2250: train loss 1.1824, val loss 1.1893\n",
            "step 2250: train loss 1.3102, val loss 1.3172\n",
            "step 2250: train loss 0.6837, val loss 0.6900\n",
            "step 2250: train loss 0.6586, val loss 0.6656\n",
            "step 2500: train loss 1.1652, val loss 1.1767\n",
            "step 2500: train loss 1.2926, val loss 1.3055\n",
            "step 2500: train loss 0.6638, val loss 0.6663\n",
            "step 2500: train loss 0.6468, val loss 0.6565\n",
            "step 2750: train loss 1.1461, val loss 1.1566\n",
            "step 2750: train loss 1.2721, val loss 1.2838\n",
            "step 2750: train loss 0.6581, val loss 0.6584\n",
            "step 2750: train loss 0.6265, val loss 0.6374\n",
            "step 3000: train loss 1.1332, val loss 1.1419\n",
            "step 3000: train loss 1.2594, val loss 1.2686\n",
            "step 3000: train loss 0.6426, val loss 0.6459\n",
            "step 3000: train loss 0.6139, val loss 0.6239\n",
            "step 3250: train loss 1.1155, val loss 1.1322\n",
            "step 3250: train loss 1.2403, val loss 1.2592\n",
            "step 3250: train loss 0.6297, val loss 0.6352\n",
            "step 3250: train loss 0.6025, val loss 0.6131\n",
            "step 3500: train loss 1.1050, val loss 1.1194\n",
            "step 3500: train loss 1.2300, val loss 1.2462\n",
            "step 3500: train loss 0.6176, val loss 0.6198\n",
            "step 3500: train loss 0.5923, val loss 0.6045\n",
            "step 3750: train loss 1.0910, val loss 1.1116\n",
            "step 3750: train loss 1.2151, val loss 1.2392\n",
            "step 3750: train loss 0.6073, val loss 0.6092\n",
            "step 3750: train loss 0.5815, val loss 0.5940\n",
            "step 4000: train loss 1.0780, val loss 1.0995\n",
            "step 4000: train loss 1.2020, val loss 1.2272\n",
            "step 4000: train loss 0.5892, val loss 0.5933\n",
            "step 4000: train loss 0.5742, val loss 0.5839\n",
            "step 4250: train loss 1.0646, val loss 1.0843\n",
            "step 4250: train loss 1.1875, val loss 1.2102\n",
            "step 4250: train loss 0.5805, val loss 0.5869\n",
            "step 4250: train loss 0.5650, val loss 0.5753\n",
            "step 4500: train loss 1.0483, val loss 1.0762\n",
            "step 4500: train loss 1.1710, val loss 1.2033\n",
            "step 4500: train loss 0.5583, val loss 0.5658\n",
            "step 4500: train loss 0.5568, val loss 0.5691\n",
            "step 4750: train loss 1.0335, val loss 1.0615\n",
            "step 4750: train loss 1.1549, val loss 1.1870\n",
            "step 4750: train loss 0.5457, val loss 0.5560\n",
            "step 4750: train loss 0.5505, val loss 0.5632\n",
            "step 4999: train loss 1.0248, val loss 1.0576\n",
            "step 4999: train loss 1.1458, val loss 1.1841\n",
            "step 4999: train loss 0.5377, val loss 0.5441\n",
            "step 4999: train loss 0.5437, val loss 0.5598\n",
            "{'new_model_start': (2025, 6, 4, 1, 45), 'memo': 'test4 re run smaller train set v2', 'batch_size': 32, 'block_size': 128, 'max_iters': 5000, 'eval_interval': 250, 'learning_rate': 0.0003, 'device': 'cpu', 'eval_iters': 200, 'n_embd': 32, 'n_head': 8, 'n_layer': 4, 'dropout': 0.2, 'max_texts': 1650, 'seq_order': ['MAIN', 'MORPH', 'POS'], 'input_dim': 3, 'main_seq_loss_contribution_weight': 0.8, 'share_train': 0.02, 'vocab_size': [37, 101, 101], 'is timestamp': False, 'limit_time_sec': 0, 'model parameters, M': 0.381039, 'model_training_start': (2025, 6, 4, 1, 54), 'reach_limit_time': False, 'step 0: train loss': 3.82293701171875, 'step 0: val loss': 3.8226778507232666, 'step 0 0: train loss': 3.6246423721313477, 'step 0 0: val loss': 3.6242477893829346, 'step 0 1: train loss': 4.624990940093994, 'step 0 1: val loss': 4.624853610992432, 'step 0 2: train loss': 4.607238292694092, 'step 0 2: val loss': 4.6079421043396, 'step 250: train loss': 2.3173470497131348, 'step 250: val loss': 2.3133699893951416, 'step 250 0: train loss': 2.4740865230560303, 'step 250 0: val loss': 2.467024326324463, 'step 250 1: train loss': 1.3457967042922974, 'step 250 1: val loss': 1.3437210321426392, 'step 250 2: train loss': 2.034977912902832, 'step 250 2: val loss': 2.0537831783294678, 'step 500: train loss': 1.940752387046814, 'step 500: val loss': 1.940374493598938, 'step 500 0: train loss': 2.141732692718506, 'step 500 0: val loss': 2.1390442848205566, 'step 500 1: train loss': 1.017544150352478, 'step 500 1: val loss': 1.0163664817810059, 'step 500 2: train loss': 1.2561182975769043, 'step 500 2: val loss': 1.2750266790390015, 'step 750: train loss': 1.6030076742172241, 'step 750: val loss': 1.5973910093307495, 'step 750 0: train loss': 1.7732287645339966, 'step 750 0: val loss': 1.7659884691238403, 'step 750 1: train loss': 0.9019718170166016, 'step 750 1: val loss': 0.8934486508369446, 'step 750 2: train loss': 0.9422734975814819, 'step 750 2: val loss': 0.9525541663169861, 'step 1000: train loss': 1.4214946031570435, 'step 1000: val loss': 1.4212143421173096, 'step 1000 0: train loss': 1.572862148284912, 'step 1000 0: val loss': 1.5712944269180298, 'step 1000 1: train loss': 0.8166329860687256, 'step 1000 1: val loss': 0.8124301433563232, 'step 1000 2: train loss': 0.8154157400131226, 'step 1000 2: val loss': 0.8293560743331909, 'step 1250: train loss': 1.3269848823547363, 'step 1250: val loss': 1.3249386548995972, 'step 1250 0: train loss': 1.4693113565444946, 'step 1250 0: val loss': 1.4664877653121948, 'step 1250 1: train loss': 0.7670470476150513, 'step 1250 1: val loss': 0.7576714158058167, 'step 1250 2: train loss': 0.7483096122741699, 'step 1250 2: val loss': 0.7598115801811218, 'step 1500: train loss': 1.2694112062454224, 'step 1500: val loss': 1.2692300081253052, 'step 1500 0: train loss': 1.4054263830184937, 'step 1500 0: val loss': 1.4040536880493164, 'step 1500 1: train loss': 0.7375349402427673, 'step 1500 1: val loss': 0.7346567511558533, 'step 1500 2: train loss': 0.713165283203125, 'step 1500 2: val loss': 0.7252140045166016, 'step 1750: train loss': 1.2333301305770874, 'step 1750: val loss': 1.238181710243225, 'step 1750 0: train loss': 1.367140531539917, 'step 1750 0: val loss': 1.371680736541748, 'step 1750 1: train loss': 0.7135549187660217, 'step 1750 1: val loss': 0.7141669392585754, 'step 1750 2: train loss': 0.6826232075691223, 'step 1750 2: val loss': 0.6942041516304016, 'step 2000: train loss': 1.2049517631530762, 'step 2000: val loss': 1.205706000328064, 'step 2000 0: train loss': 1.33608078956604, 'step 2000 0: val loss': 1.3355648517608643, 'step 2000 1: train loss': 0.6940873861312866, 'step 2000 1: val loss': 0.6948351860046387, 'step 2000 2: train loss': 0.6667835712432861, 'step 2000 2: val loss': 0.6777039170265198, 'step 2250: train loss': 1.1823617219924927, 'step 2250: val loss': 1.18929922580719, 'step 2250 0: train loss': 1.310153841972351, 'step 2250 0: val loss': 1.317175030708313, 'step 2250 1: train loss': 0.6837437152862549, 'step 2250 1: val loss': 0.6900144219398499, 'step 2250 2: train loss': 0.6586434245109558, 'step 2250 2: val loss': 0.6655786037445068, 'step 2500: train loss': 1.1651508808135986, 'step 2500: val loss': 1.1766674518585205, 'step 2500 0: train loss': 1.2926080226898193, 'step 2500 0: val loss': 1.3054803609848022, 'step 2500 1: train loss': 0.6638475656509399, 'step 2500 1: val loss': 0.6663012504577637, 'step 2500 2: train loss': 0.6467956304550171, 'step 2500 2: val loss': 0.6565313935279846, 'step 2750: train loss': 1.1461001634597778, 'step 2750: val loss': 1.156640887260437, 'step 2750 0: train loss': 1.2720593214035034, 'step 2750 0: val loss': 1.283828854560852, 'step 2750 1: train loss': 0.6580725908279419, 'step 2750 1: val loss': 0.6583898663520813, 'step 2750 2: train loss': 0.6264533400535583, 'step 2750 2: val loss': 0.6373862624168396, 'step 3000: train loss': 1.1331524848937988, 'step 3000: val loss': 1.1418828964233398, 'step 3000 0: train loss': 1.2593789100646973, 'step 3000 0: val loss': 1.2686266899108887, 'step 3000 1: train loss': 0.6426236629486084, 'step 3000 1: val loss': 0.6458870768547058, 'step 3000 2: train loss': 0.6138705015182495, 'step 3000 2: val loss': 0.6239286661148071, 'step 3250: train loss': 1.115476369857788, 'step 3250: val loss': 1.1321914196014404, 'step 3250 0: train loss': 1.2403219938278198, 'step 3250 0: val loss': 1.259202003479004, 'step 3250 1: train loss': 0.629697859287262, 'step 3250 1: val loss': 0.6351587176322937, 'step 3250 2: train loss': 0.6024899482727051, 'step 3250 2: val loss': 0.613138735294342, 'step 3500: train loss': 1.1049747467041016, 'step 3500: val loss': 1.1193732023239136, 'step 3500 0: train loss': 1.2299766540527344, 'step 3500 0: val loss': 1.2461836338043213, 'step 3500 1: train loss': 0.6175930500030518, 'step 3500 1: val loss': 0.6197934746742249, 'step 3500 2: train loss': 0.5923401713371277, 'step 3500 2: val loss': 0.6044696569442749, 'step 3750: train loss': 1.0909945964813232, 'step 3750: val loss': 1.1116350889205933, 'step 3750 0: train loss': 1.2151423692703247, 'step 3750 0: val loss': 1.239151120185852, 'step 3750 1: train loss': 0.6072967052459717, 'step 3750 1: val loss': 0.6091705560684204, 'step 3750 2: train loss': 0.5815097093582153, 'step 3750 2: val loss': 0.593971312046051, 'step 4000: train loss': 1.0779600143432617, 'step 4000: val loss': 1.0994759798049927, 'step 4000 0: train loss': 1.202020287513733, 'step 4000 0: val loss': 1.2272040843963623, 'step 4000 1: train loss': 0.5892329812049866, 'step 4000 1: val loss': 0.59325110912323, 'step 4000 2: train loss': 0.5742045044898987, 'step 4000 2: val loss': 0.5838741064071655, 'step 4250: train loss': 1.0645570755004883, 'step 4250: val loss': 1.084346055984497, 'step 4250 0: train loss': 1.1875072717666626, 'step 4250 0: val loss': 1.2101526260375977, 'step 4250 1: train loss': 0.5805408954620361, 'step 4250 1: val loss': 0.5868945717811584, 'step 4250 2: train loss': 0.5649718642234802, 'step 4250 2: val loss': 0.5753446817398071, 'step 4500: train loss': 1.048313856124878, 'step 4500: val loss': 1.0761723518371582, 'step 4500 0: train loss': 1.171013355255127, 'step 4500 0: val loss': 1.2033463716506958, 'step 4500 1: train loss': 0.5582624673843384, 'step 4500 1: val loss': 0.5658126473426819, 'step 4500 2: train loss': 0.5567706227302551, 'step 4500 2: val loss': 0.5691403746604919, 'step 4750: train loss': 1.033542275428772, 'step 4750: val loss': 1.0615005493164062, 'step 4750 0: train loss': 1.1549084186553955, 'step 4750 0: val loss': 1.1869821548461914, 'step 4750 1: train loss': 0.5456960797309875, 'step 4750 1: val loss': 0.5559958219528198, 'step 4750 2: train loss': 0.550459623336792, 'step 4750 2: val loss': 0.5631527900695801, 'step 4999: train loss': 1.0248016119003296, 'step 4999: val loss': 1.0576467514038086, 'step 4999 0: train loss': 1.145827293395996, 'step 4999 0: val loss': 1.1840691566467285, 'step 4999 1: train loss': 0.5377267003059387, 'step 4999 1: val loss': 0.544133722782135, 'step 4999 2: train loss': 0.5436714291572571, 'step 4999 2: val loss': 0.5597812533378601, 'model_end': (2025, 6, 4, 3, 12)}\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "# ------ hyperparameters ------------\n",
        "memo = \"test1\"\n",
        "batch_size = 32\n",
        "block_size = 128\n",
        "max_iters =  5000\n",
        "eval_interval = 250\n",
        "learning_rate = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 56 #IMPORTANT n_embd // n_head\n",
        "n_head = 8\n",
        "n_layer = 10\n",
        "dropout = 0.2\n",
        "seq_order = ['MAIN']\n",
        "input_dim = len(seq_order)\n",
        "max_texts = 1650\n",
        "main_seq_loss_contribution_weight = 0.8\n",
        "share_train = 0.9\n",
        "# ------------------------------------\n",
        "def test_run(_break = False, _timestamp = False, limit_time_sec = 0):\n",
        "  _save_needed = True\n",
        "\n",
        "  w = Workflow()\n",
        "\n",
        "  w.log[\"new_model_start\"] = (datetime.datetime.now().year,\n",
        "                              datetime.datetime.now().month,\n",
        "                              datetime.datetime.now().day,\n",
        "                              datetime.datetime.now().hour,\n",
        "                              datetime.datetime.now().minute)\n",
        "\n",
        "  w.data_prep(max_text=max_texts, load_seq = seq_order)\n",
        "\n",
        "  w.log[\"memo\"] = memo\n",
        "  w.log[\"batch_size\"] = batch_size\n",
        "  w.log[\"block_size\"] = block_size\n",
        "  w.log[\"max_iters\"] = max_iters\n",
        "  w.log[\"eval_interval\"] = eval_interval\n",
        "  w.log[\"learning_rate\"] = learning_rate\n",
        "  w.log[\"device\"] = device\n",
        "  w.log[\"eval_iters\"] = eval_iters\n",
        "  w.log[\"n_embd\"] = n_embd\n",
        "  w.log[\"n_head\"] = n_head\n",
        "  w.log[\"n_layer\"] = n_layer\n",
        "  w.log[\"dropout\"] = dropout\n",
        "  w.log[\"max_texts\"] = max_texts\n",
        "  w.log[\"seq_order\"] = seq_order\n",
        "  w.log[\"input_dim\"] = input_dim\n",
        "  w.log[\"main_seq_loss_contribution_weight\"] = main_seq_loss_contribution_weight\n",
        "  w.log[\"share_train\"] = share_train\n",
        "  w.log[\"vocab_size\"] = w.vocab_sizes\n",
        "  w.log[\"is timestamp\"] = _timestamp\n",
        "  w.log[\"limit_time_sec\"] = limit_time_sec\n",
        "\n",
        "  model = GPTLanguageModel(w.vocab_sizes)\n",
        "  m = model.to(device)\n",
        "  m_parameters = sum(p.numel() for p in m.parameters())/1e6\n",
        "  print(m_parameters, 'M parameters')\n",
        "  if _break: return\n",
        "  w.log[\"model parameters, M\"] = m_parameters\n",
        "  optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "  w.log[\"model_training_start\"] = (datetime.datetime.now().year,\n",
        "                              datetime.datetime.now().month,\n",
        "                              datetime.datetime.now().day,\n",
        "                              datetime.datetime.now().hour,\n",
        "                              datetime.datetime.now().minute)\n",
        "\n",
        "  if _timestamp: time_start = time.time()\n",
        "\n",
        "  w.log[\"reach_limit_time\"] = False\n",
        "\n",
        "  for iter in range(max_iters):\n",
        "      if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "          losses, lossess_detailed = estimate_loss(model, w)\n",
        "          print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "          w.log[str(f\"step {iter}: train loss\")] = float(losses['train'])\n",
        "          w.log[str(f\"step {iter}: val loss\")] = float(losses['val'])\n",
        "          for i in range(input_dim):\n",
        "            print(f\"step {iter}: train loss {lossess_detailed['train'][i]:.4f}, val loss {lossess_detailed['val'][i]:.4f}\")\n",
        "            w.log[str(f\"step {iter} {i}: train loss\")] = float(lossess_detailed['train'][i])\n",
        "            w.log[str(f\"step {iter} {i}: val loss\")] = float(lossess_detailed['val'][i])\n",
        "          if _timestamp and time.time() - time_start > limit_time_sec:\n",
        "            w.log[\"reach_limit_time\"] = True\n",
        "            break\n",
        "      x, y = w.get_batch('train')\n",
        "      (logits, logits_w), loss, individual_losses = model(x, y)\n",
        "      optimizer.zero_grad(set_to_none=True)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "  w.log[\"model_end\"] = (datetime.datetime.now().year,\n",
        "                              datetime.datetime.now().month,\n",
        "                              datetime.datetime.now().day,\n",
        "                              datetime.datetime.now().hour,\n",
        "                              datetime.datetime.now().minute)\n",
        "  print(w.log)\n",
        "  if _save_needed: save_dict_to_csv_gdrive(w.log, 'log_data.csv')\n",
        "\n",
        "_early_break = False\n",
        "\n",
        "run_tag = \" re run smaller train set v2\"\n",
        "\n",
        "# ------ hyperparameters ------------\n",
        "memo = \"test1\" + run_tag\n",
        "share_train = 0.02\n",
        "seq_order = ['MAIN']\n",
        "input_dim = len(seq_order)\n",
        "# ------------------------------------\n",
        "test_run(_break = _early_break)\n",
        "\n",
        "# ------ hyperparameters ------------\n",
        "memo = \"test2\" + run_tag\n",
        "n_embd = 32\n",
        "n_layer = 7\n",
        "seq_order = ['MAIN','MORPH']\n",
        "input_dim = len(seq_order)\n",
        "# ------------------------------------\n",
        "test_run(_break = _early_break)\n",
        "\n",
        "# ------ hyperparameters ------------\n",
        "memo = \"test3\" + run_tag\n",
        "seq_order = ['MAIN','POS']\n",
        "input_dim = len(seq_order)\n",
        "# ------------------------------------\n",
        "test_run(_break = _early_break)\n",
        "\n",
        "# ------ hyperparameters ------------\n",
        "memo = \"test4\" + run_tag\n",
        "n_layer = 4\n",
        "seq_order = ['MAIN','MORPH','POS']\n",
        "input_dim = len(seq_order)\n",
        "# ------------------------------------\n",
        "test_run(_break = _early_break)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQ9LqrUbqWu_"
      },
      "outputs": [],
      "source": [
        "#---- longer run\n",
        "\n",
        "run_tag = \" longer run\"\n",
        "memo = \"test1\" + run_tag\n",
        "\n",
        "test_run(_break = _early_break, _timestamp = True, limit_time_sec = 60*90)\n",
        "\n",
        "# ------ hyperparameters ------------\n",
        "memo = \"test2\" + run_tag\n",
        "n_embd = 168\n",
        "n_layer = 11\n",
        "seq_order = ['MAIN']\n",
        "input_dim = len(seq_order)\n",
        "# ------------------------------------\n",
        "test_run(_break = _early_break, _timestamp = True, limit_time_sec = 5*60*60)\n",
        "\n",
        "# ---- larger model\n",
        "\n",
        "run_tag = \" larger models\"\n",
        "\n",
        "# ------ hyperparameters ------------\n",
        "memo = \"test1\" + run_tag\n",
        "n_embd = 56*3\n",
        "n_layer = 3*3+2\n",
        "seq_order = ['MAIN']\n",
        "input_dim = len(seq_order)\n",
        "# ------------------------------------\n",
        "test_run(_break = _early_break)\n",
        "\n",
        "# ------ hyperparameters ------------\n",
        "memo = \"test2\" + run_tag\n",
        "n_embd = 32*3\n",
        "n_layer = 2*3+2\n",
        "seq_order = ['MAIN','MORPH']\n",
        "input_dim = len(seq_order)\n",
        "# ------------------------------------\n",
        "test_run(_break = _early_break)\n",
        "\n",
        "# ------ hyperparameters ------------\n",
        "memo = \"test3\" + run_tag\n",
        "seq_order = ['MAIN','POS']\n",
        "input_dim = len(seq_order)\n",
        "# ------------------------------------\n",
        "test_run(_break = _early_break)\n",
        "\n",
        "# ------ hyperparameters ------------\n",
        "memo = \"test4\" + run_tag\n",
        "n_embd = 24*3\n",
        "n_layer = 2*3+2\n",
        "seq_order = ['MAIN','MORPH','POS']\n",
        "input_dim = len(seq_order)\n",
        "# ------------------------------------\n",
        "test_run(_break = _early_break)\n",
        "\n",
        "# ---- smaller model\n",
        "\n",
        "run_tag = \" smaller models\"\n",
        "\n",
        "# ------ hyperparameters ------------\n",
        "memo = \"test1\" + run_tag\n",
        "n_embd = 56\n",
        "n_layer = 3\n",
        "seq_order = ['MAIN']\n",
        "input_dim = len(seq_order)\n",
        "# ------------------------------------\n",
        "#test_run(_break = _early_break)\n",
        "\n",
        "# ------ hyperparameters ------------\n",
        "memo = \"test2\" + run_tag\n",
        "n_embd = 32\n",
        "n_layer = 2\n",
        "seq_order = ['MAIN','MORPH']\n",
        "input_dim = len(seq_order)\n",
        "# ------------------------------------\n",
        "#test_run(_break = _early_break)\n",
        "\n",
        "# ------ hyperparameters ------------\n",
        "memo = \"test3\" + run_tag\n",
        "seq_order = ['MAIN','POS']\n",
        "input_dim = len(seq_order)\n",
        "# ------------------------------------\n",
        "#test_run(_break = _early_break)\n",
        "\n",
        "# ------ hyperparameters ------------\n",
        "memo = \"test4\" + run_tag\n",
        "n_embd = 24\n",
        "n_layer = 2\n",
        "seq_order = ['MAIN','MORPH','POS']\n",
        "input_dim = len(seq_order)\n",
        "# ------------------------------------\n",
        "test_run(_break = _early_break)\n",
        "\n",
        "# ---- new share train - 0.5\n",
        "\n",
        "# ------ hyperparameters ------------\n",
        "memo = \"test1\"\n",
        "n_embd = 56\n",
        "n_layer = 8\n",
        "share_train = 0.5\n",
        "seq_order = ['MAIN']\n",
        "input_dim = len(seq_order)\n",
        "# ------------------------------------\n",
        "test_run(_break = _early_break)\n",
        "\n",
        "# ------ hyperparameters ------------\n",
        "memo = \"test2\"\n",
        "n_embd = 32\n",
        "n_layer = 7\n",
        "seq_order = ['MAIN','MORPH']\n",
        "input_dim = len(seq_order)\n",
        "# ------------------------------------\n",
        "test_run(_break = _early_break)\n",
        "\n",
        "# ------ hyperparameters ------------\n",
        "memo = \"test3\"\n",
        "seq_order = ['MAIN','POS']\n",
        "input_dim = len(seq_order)\n",
        "# ------------------------------------\n",
        "test_run(_break = _early_break)\n",
        "\n",
        "# ------ hyperparameters ------------\n",
        "memo = \"test4\"\n",
        "n_embd = 32\n",
        "n_layer = 4\n",
        "seq_order = ['MAIN','MORPH','POS']\n",
        "input_dim = len(seq_order)\n",
        "# ------------------------------------\n",
        "test_run(_break = _early_break)\n",
        "\n",
        "# ---- new share train - 0.2\n",
        "\n",
        "# ------ hyperparameters ------------\n",
        "memo = \"test1\"\n",
        "n_embd = 56\n",
        "n_layer = 8\n",
        "share_train = 0.2\n",
        "seq_order = ['MAIN']\n",
        "input_dim = len(seq_order)\n",
        "# ------------------------------------\n",
        "test_run(_break = _early_break)\n",
        "\n",
        "# ------ hyperparameters ------------\n",
        "memo = \"test2\"\n",
        "n_embd = 32\n",
        "n_layer = 7\n",
        "seq_order = ['MAIN','MORPH']\n",
        "input_dim = len(seq_order)\n",
        "# ------------------------------------\n",
        "test_run(_break = _early_break)\n",
        "\n",
        "# ------ hyperparameters ------------\n",
        "memo = \"test3\"\n",
        "seq_order = ['MAIN','POS']\n",
        "input_dim = len(seq_order)\n",
        "# ------------------------------------\n",
        "test_run(_break = _early_break)\n",
        "\n",
        "# ------ hyperparameters ------------\n",
        "memo = \"test4\"\n",
        "n_embd = 32\n",
        "n_layer = 4\n",
        "seq_order = ['MAIN','MORPH','POS']\n",
        "input_dim = len(seq_order)\n",
        "# ------------------------------------\n",
        "test_run(_break = _early_break)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QEpRaxeTxfdV",
        "outputId": "dbe4f058-65cf-45c0-a50e-e2d6127bec58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wed May 21 02:31:30 AM UTC 2025\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "2 texts loaded\n",
            "4 texts loaded\n",
            "6 texts loaded\n",
            "8 texts loaded\n",
            "10 texts loaded\n",
            "0\n",
            "0\n",
            "50\n",
            "0.004298 M parameters\n",
            "step 0: train loss 3.8152, val loss 3.8155\n",
            "step 0: train loss 3.6153, val loss 3.6156\n",
            "step 0: train loss 4.6148, val loss 4.6149\n",
            "step 250: train loss 3.4172, val loss 3.4134\n",
            "step 250: train loss 3.2742, val loss 3.2700\n",
            "step 250: train loss 3.9891, val loss 3.9872\n",
            "step 499: train loss 3.0772, val loss 3.0694\n",
            "step 499: train loss 3.0425, val loss 3.0349\n",
            "step 499: train loss 3.2160, val loss 3.2071\n",
            "{'new_model_start': (2025, 5, 21, 2, 31), 'memo': 'test', 'batch_size': 4, 'block_size': 32, 'max_iters': 500, 'eval_interval': 250, 'learning_rate': 0.0003, 'device': 'cpu', 'eval_iters': 200, 'n_embd': 4, 'n_head': 2, 'n_layer': 3, 'dropout': 0.2, 'max_texts': 10, 'seq_order': ['MAIN', 'MORPH'], 'input_dim': 2, 'main_seq_loss_contribution_weight': 0.8, 'vocab_size': [37, 101], 'step 0: train loss': 3.815183162689209, 'step 0: val loss': 3.815485954284668, 'step 0 0: train loss': 3.615266799926758, 'step 0 0: val loss': 3.615626335144043, 'step 0 1: train loss': 4.614847183227539, 'step 0 1: val loss': 4.614923000335693, 'step 250: train loss': 3.417154550552368, 'step 250: val loss': 3.4134292602539062, 'step 250 0: train loss': 3.274172782897949, 'step 250 0: val loss': 3.2699978351593018, 'step 250 1: train loss': 3.989081382751465, 'step 250 1: val loss': 3.9871561527252197, 'step 499: train loss': 3.0772085189819336, 'step 499: val loss': 3.0693519115448, 'step 499 0: train loss': 3.0425074100494385, 'step 499 0: val loss': 3.034902334213257, 'step 499 1: train loss': 3.2160134315490723, 'step 499 1: val loss': 3.207149028778076, 'model_end': (2025, 5, 21, 2, 31)}\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# ------ hyperparameters ------------\n",
        "batch_size = 4#64#32#16 #8 #64 #\n",
        "block_size = 32#128#256#128#64 #128 #256 #\n",
        "max_iters =  500#40000 # 5000 #1000 #5000\n",
        "eval_interval = 250#1000 #100 #500\n",
        "learning_rate = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 4#16#64#32#16# 8 #384. IMPORTANT n_embd // n_head\n",
        "n_head = 2#6#4# 2 #6\n",
        "n_layer = 3#7 #6\n",
        "dropout = 0.2\n",
        "seq_order = ['MAIN','MORPH']#,'POS']\n",
        "input_dim = len(seq_order)\n",
        "max_texts = 10 #1700\n",
        "main_seq_loss_contribution_weight = 0.8\n",
        "share_train = 0.9 #TODO Test less data needed\n",
        "\n",
        "# ------------------------------------\n",
        "\n",
        "_save_needed = True\n",
        "\n",
        "w = Workflow()\n",
        "\n",
        "w.log[\"new_model_start\"] = (datetime.datetime.now().year,\n",
        "                            datetime.datetime.now().month,\n",
        "                            datetime.datetime.now().day,\n",
        "                            datetime.datetime.now().hour,\n",
        "                            datetime.datetime.now().minute)\n",
        "\n",
        "w.data_prep(max_text=max_texts, load_seq = seq_order)\n",
        "\n",
        "w.log[\"memo\"] = \"test\"\n",
        "w.log[\"batch_size\"] = batch_size\n",
        "w.log[\"block_size\"] = block_size\n",
        "w.log[\"max_iters\"] = max_iters\n",
        "w.log[\"eval_interval\"] = eval_interval\n",
        "w.log[\"learning_rate\"] = learning_rate\n",
        "w.log[\"device\"] = device\n",
        "w.log[\"eval_iters\"] = eval_iters\n",
        "w.log[\"n_embd\"] = n_embd\n",
        "w.log[\"n_head\"] = n_head\n",
        "w.log[\"n_layer\"] = n_layer\n",
        "w.log[\"dropout\"] = dropout\n",
        "w.log[\"max_texts\"] = max_texts\n",
        "w.log[\"seq_order\"] = seq_order\n",
        "w.log[\"input_dim\"] = input_dim\n",
        "w.log[\"main_seq_loss_contribution_weight\"] = main_seq_loss_contribution_weight\n",
        "\n",
        "w.log[\"vocab_size\"] = w.vocab_sizes\n",
        "\n",
        "model = GPTLanguageModel(w.vocab_sizes)\n",
        "m = model.to(device)\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses, lossess_detailed = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "        w.log[str(f\"step {iter}: train loss\")] = float(losses['train'])\n",
        "        w.log[str(f\"step {iter}: val loss\")] = float(losses['val'])\n",
        "        for i in range(input_dim):\n",
        "          print(f\"step {iter}: train loss {lossess_detailed['train'][i]:.4f}, val loss {lossess_detailed['val'][i]:.4f}\")\n",
        "          w.log[str(f\"step {iter} {i}: train loss\")] = float(lossess_detailed['train'][i])\n",
        "          w.log[str(f\"step {iter} {i}: val loss\")] = float(lossess_detailed['val'][i])\n",
        "    x, y = w.get_batch('train')\n",
        "    (logits, logits_w), loss, individual_losses = model(x, y)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "w.log[\"model_end\"] = (datetime.datetime.now().year,\n",
        "                            datetime.datetime.now().month,\n",
        "                            datetime.datetime.now().day,\n",
        "                            datetime.datetime.now().hour,\n",
        "                            datetime.datetime.now().minute)\n",
        "print(w.log)\n",
        "if _save_needed: save_dict_to_csv_gdrive(w.log, 'log_data.csv')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V5E1",
      "provenance": [],
      "authorship_tag": "ABX9TyOLyLQvqI2u/OUSkvNNSH84",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}